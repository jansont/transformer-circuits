{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D06wUjxTZ8ej",
    "outputId": "ee034021-4479-432b-8a21-de6d34adb14a",
    "tags": []
   },
   "source": [
    "# Causal Analysis of Facts\n",
    "We analyze where facts might be locateed in an LLM. A fact is represented by a tuple of subject, relation, object (s,r,o). We also investigate where an inverse fact mis located. An inverse fact is represented by the tuple (o, r^-1, s). For example, the fact \"Paris is the capital of France\" has the inverse fact \"France's capital is France\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": null,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    # %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "    renderer = \"colab\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from IPython import get_ipython\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    renderer = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = False\n",
    "renderer = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< HEAD
=======
   "execution_count": 2,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
      "Requirement already satisfied: poetry in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: build<0.11.0,>=0.10.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.10.0)\n",
      "Requirement already satisfied: cachecontrol[filecache]<0.13.0,>=0.12.9 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.12.14)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.0.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.21.5)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.8.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (3.12.2)\n",
      "Requirement already satisfied: html5lib<2.0,>=1.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (1.1)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.10.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (4.17.3)\n",
      "Requirement already satisfied: keyring<24.0.0,>=23.9.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (23.13.1)\n",
      "Requirement already satisfied: lockfile<0.13.0,>=0.12.2 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.12.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (23.1)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (3.8.0)\n",
      "Requirement already satisfied: poetry-core==1.6.1 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (1.6.1)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.4.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (1.4.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.18 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (1.5.2)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.11.8)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (2023.8.7)\n",
      "Collecting urllib3<2.0.0,>=1.26.0 (from poetry)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.22.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (20.24.3)\n",
      "Requirement already satisfied: xattr<0.11.0,>=0.10.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from poetry) (0.10.1)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from cachecontrol[filecache]<0.13.0,>=0.12.9->poetry) (1.0.5)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.2.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from cleo<3.0.0,>=2.0.0->poetry) (2.15.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (0.19.3)\n",
      "Requirement already satisfied: jaraco.classes in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (5.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (2023.5.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from virtualenv<21.0.0,>=20.22.0->poetry) (0.3.7)\n",
<<<<<<< HEAD
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from xattr<0.11.0,>=0.10.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from cffi>=1.0->xattr<0.11.0,>=0.10.0->poetry) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<24.0.0,>=23.9.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: more-itertools in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from jaraco.classes->keyring<24.0.0,>=23.9.0->poetry) (10.1.0)\n",
      "Installing collected packages: urllib3, platformdirs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
=======
      "Requirement already satisfied: poetry in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: build<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.10.0)\n",
      "Requirement already satisfied: cachecontrol[filecache]<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.13.1)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.21.5)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: jsonschema<4.18.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.17.3)\n",
      "Requirement already satisfied: keyring<25.0.0,>=24.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (24.2.0)\n",
      "Requirement already satisfied: packaging>=20.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.1)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.8.0)\n",
      "Requirement already satisfied: poetry-core==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.7.0)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.11.8)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2023.8.7)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (20.24.3)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (1.0.5)\n",
      "Requirement already satisfied: filelock>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (3.12.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cleo<3.0.0,>=2.0.0->poetry) (2.15.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dulwich<0.22.0,>=0.21.2->poetry) (2.0.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<4.18.0,>=4.10.0->poetry) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<4.18.0,>=4.10.0->poetry) (0.19.3)\n",
      "Requirement already satisfied: jaraco.classes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (5.2.0)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.26->poetry) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.26->poetry) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.26->poetry) (2023.5.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from virtualenv<21.0.0,>=20.22.0->poetry) (0.3.7)\n",
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Obtaining dependency information for platformdirs<4.0.0,>=3.0.0 from https://files.pythonhosted.org/packages/14/51/fe5a0d6ea589f0d4a1b97824fb518962ad48b27cd346dcdfa2405187997a/platformdirs-3.10.0-py3-none-any.whl.metadata\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (41.0.2)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry) (10.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (2.21)\n",
      "Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: platformdirs\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from xattr<0.11.0,>=0.10.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from cffi>=1.0->xattr<0.11.0,>=0.10.0->poetry) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<24.0.0,>=23.9.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: more-itertools in /Users/theojanson/opt/anaconda3/envs/transformer-circuits/lib/python3.10/site-packages (from jaraco.classes->keyring<24.0.0,>=23.9.0->poetry) (10.1.0)\n",
      "Installing collected packages: urllib3, platformdirs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.8.0\n",
      "    Uninstalling platformdirs-3.8.0:\n",
      "      Successfully uninstalled platformdirs-3.8.0\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "Successfully installed platformdirs-3.10.0 urllib3-1.26.16\n",
      "Installing dependencies from lock file\n",
      "\n",
      "Package operations: 0 installs, 2 updates, 0 removals\n",
      "\n",
      "  • Updating urllib3 (1.26.16 -> 2.0.3)\n",
      "  • Updating platformdirs (3.10.0 -> 3.8.0)\n",
      "\n",
      "Installing the current project: transformer-lens (0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration file exists at /Users/theojanson/Library/Preferences/pypoetry, reusing this directory.\n",
      "\n",
      "Consider moving TOML configuration files to /Users/theojanson/Library/Application Support/pypoetry, as support for the legacy directory will be removed in an upcoming release.\n"
     ]
=======
      "Successfully installed platformdirs-3.10.0\n",
=======
      "Successfully installed platformdirs-3.10.0 urllib3-1.26.16\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
      "Installing dependencies from lock file\n",
      "\n",
      "Package operations: 0 installs, 2 updates, 0 removals\n",
      "\n",
      "  • Updating urllib3 (1.26.16 -> 2.0.3)\n",
      "  • Updating platformdirs (3.10.0 -> 3.8.0)\n",
      "\n",
      "Installing the current project: transformer-lens (0.0.0)\n"
     ]
<<<<<<< HEAD
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration file exists at /Users/theojanson/Library/Preferences/pypoetry, reusing this directory.\n",
      "\n",
      "Consider moving TOML configuration files to /Users/theojanson/Library/Application Support/pypoetry, as support for the legacy directory will be removed in an upcoming release.\n"
     ]
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../\n",
    "pip install poetry\n",
    "poetry install\n",
    "cd notebooks"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": null,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "id": "SXYnmmdiKv4V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = renderer"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 2,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "id": "epoi7Cb7Kv4V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import stuff\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "import subprocess\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 176,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 3,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c5KcrsoJKv4W",
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "# if IN_COLAB: \n",
    "#     import pysvelte\n",
=======
    "if IN_COLAB: \n",
    "    import pysvelte\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "# if IN_COLAB: \n",
    "#     import pysvelte\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens.utilities import devices\n",
    "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdw4b_oKKv4W"
   },
   "source": [
    "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbmPPShhKv4W",
    "outputId": "f71a265a-00c8-427e-dc22-a5f8a5f6e056",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
<<<<<<< HEAD
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb87957f280>"
=======
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6eb71d1ab0>"
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb87957f280>"
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F83ndvw3Kv4W"
   },
   "source": [
    "Plotting helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9QMgUkYMKv4W",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
<<<<<<< HEAD
       "'cpu'"
=======
       "'cuda'"
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
       "'cpu'"
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "def cuda():\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "def get_device(): \n",
    "    return \"cuda\" if cuda() else \"cpu\"\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Function to load a pickle object from a file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "    return loaded_data\n",
    "\n",
    "def get_gpu_usage(): \n",
    "    subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "- fold_ln: Whether to fold in the LayerNorm weights to the subsequent linear layer. This does not change the computation.\n",
    "- center_writing_weights: Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNormthis doesn't change the computation.\n",
    "- center_unembed : Whether to center W_U (ie set mean to be zero). Softmax is translation invariant so this doesn't affect log probs or loss, but does change logits. Defaults to True.\n",
    "- refactor_factored_attn_matrices: Whether to convert the factoredmatrices (W_Q & W_K, and W_O & W_V) to be \"even\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "d5c7b0d2a6ff43eca128a12113300bd9",
      "5553fd97122a493fb99e3af859d16ddf",
      "4dd5d713b82742df9b120193752b769c",
      "340a9c92995a4189b69e459b033ae946",
      "89cc2257d1ea4e63a889c0c4c3b20b62",
      "4a08436dc4c54a1ab2219e66058f68b5",
      "e75523e3b32a449bb5f08ab49d57a473",
      "50fd7bca1a95484fa13c37242a988fc3",
      "6309b4c1e92f46e9b09f9615d20705d8",
      "d3419184787645f9b617dc18ccaf315a",
      "45df25e66c6041d6b0470865f43d3030",
      "7e9d95a57d194b379f33467895fc8883",
      "7eecaf2e1f6d401b9d85abf1438cde9b",
      "981666d6021845389f9fa1a0ff5c7997",
      "d54bba9da6d54cbd8e83eec29dae02c4",
      "2df76b7b518948d88d9aba8f670e7f9c",
      "c2ce3a86d5b34b2eb263fedcbbce72a0",
      "4baa43529767484fb04fb1f9e94c1aab",
      "1198949157d045b4b1d56330c21f861b",
      "32421abdc8fa41d8917203f6bfad586d",
      "05c094717d594d37985f58c51527db59",
      "9704b43fd02d47be994fefadbde6dfa2",
      "57a96c26b3944e7486bb2ae8c26d8973",
      "e5075a3f50b34526bdd4e215488341af",
      "0632336b81a043d09cab7a8b3774968c",
      "b5defcb6637e4847a76bb982bd40dba4",
      "c21ba829e49b4824a83d9e79ec5b9892",
      "5f7b0750af3646b787026494a1872a1f",
      "6586f332f4104d8d914b8d390049310b",
      "0f3cef4147924e129b11161067776df0",
      "03055d1e38844792bbd9750b2daf786a",
      "6208163737e24a8ebc66fa95c7f4217f",
      "30e53c8500a14fb880c2ab52007d1087",
      "fba7c9b3bf9b409a9166cae7fc5fdba1",
      "1c3f06e311c648569f17cf18f9c2f4f3",
      "c55ebb7c205d40ba8c9f2682dd169f37",
      "80bce29dfc73463c9b4cbeaea636ed74",
      "4193f94efd9e457483fc4a2b03a3c07d",
      "95713e453b7945d4bf3980f1ae512033",
      "d767ae133f1a47bab5a40f4d8ac0d272",
      "1ff7229e899e4afab9b5ee0b1597f5e5",
      "08b2a881d8b44bca9eabbe25558d2b10",
      "e240513c73c2418f835465c3a93683f2",
      "576faf57e7234808866d8ae0c273ecbd",
      "9b37ca3bdb944f2b95e65fdb4412e7dd",
      "eb702efbe48642e1b9dd743faa0fcecf",
      "c73ee2d1b39d4eeb90e976ee9710f859",
      "13e8123052ad4599829fc081dac4eab3",
      "272adc9fc3ba4c07943725eb800a18c1",
      "a90d112104994987b228ec0d4c7cd419",
      "ffb3be45dd1a4454bf59bb7ffb1e2413",
      "f073c1fbf7724c689cfe92ed50e43254",
      "d666d404db714d7f94a67d434b54a22b",
      "53ab2b48537d45f0a311d51bdf2f03ff",
      "f36e03e930584b7d821b4184c54a5810",
      "0336fd949a66477c9daaca7b78b5a3a3",
      "669e15f191394c0ca74630c025b164bc",
      "f32ea620bbc342dab3179e632f8fbe44",
      "d276c9530d9f456e9ae9c0b3a931cb93",
      "ea6138d3d54f483587595bdda5211215",
      "d1d36249ca9b430dad8d8327138b8389",
      "a865c5a49b0f4e84a6621b16d682edbd",
      "b2755e69b5484a28ade3d560840aecc5",
      "7413bf18522241d3807a01f016d6139e",
      "ce75e14ca9384d04bb297b6be28c36e5",
      "267fca47895e493e82c9ef0568847b18"
     ]
    },
    "id": "oyrZcBKMecVw",
    "outputId": "35c1fbee-6dad-4eb8-a7b1-e67cbb5497ac",
    "tags": []
   },
   "outputs": [
    {
<<<<<<< HEAD
<<<<<<< HEAD
=======
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b414ef67b3d44bca16b8860be22df7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5ed832508341dcb8df85d206164725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5a763c28e34e8dba27d03144b88a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f49c29dd9484d4bb53dc2a0593eb5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76934695ee2466b8ad595548b9742a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723d422be0f74391bcd54d9ffe7d3638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-small\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        center_unembed=True,  \n",
    "        center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "        fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use the CounterFact Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_json(\"data/fact_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, idx = None): \n",
    "    if idx is None: \n",
    "        prompt = \"The {} is located in\"\n",
    "        subject = \"Eiffel Tower\"\n",
    "        target = \"Paris\"\n",
    "    else: \n",
    "        sample = dataset[idx]\n",
    "        prompt = sample[\"requested_rewrite\"][\"prompt\"]\n",
    "        subject = sample[\"requested_rewrite\"][\"subject\"]\n",
    "        target = sample[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "    return prompt, subject, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = sample_dataset(dataset)\n",
    "prompt, subject, target = sample\n",
    "prompt = prompt.format(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(prompt).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Methods: \n",
    "- Noise ablation \n",
    "- Resample ablation"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 22,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 14,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def noise_ablation(prompt, subject, target, n_noise_samples=5, vx=3):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    \n",
    "    #shape: batch, n_tokens, embedding_dim\n",
    "    subject_embedding = model.embed(subject_tokens)\n",
    "    _, n_tokens, embedding_dim = subject_embedding.shape\n",
    "    \n",
    "    #noise: N(0,v), v = 3*std(embedding)\n",
    "    embedding = model.W_E\n",
    "    v = vx*torch.std(embedding, dim=0) #for each v in V\n",
    "    noise = torch.randn(\n",
    "        (n_noise_samples, n_tokens, embedding_dim)\n",
    "    ).to(device) + v\n",
    "    \n",
    "    subject_embedding_w_noise = subject_embedding + noise\n",
    "    \n",
    "    #shape: batch, n_tokens, vocab_size (logits)\n",
    "    unembedded_subject = model.unembed(subject_embedding_w_noise)\n",
    "\n",
    "    noisy_subject_tokens = torch.argmax(unembedded_subject, dim=-1)\n",
    "    noisy_subject_str = [\n",
    "        model.to_string(nst) for nst in noisy_subject_tokens\n",
    "    ]\n",
    "    true_prompt = prompt.format(subject)\n",
    "    corrupted_prompts = [\n",
    "        prompt.format(nss.strip()) for nss in noisy_subject_str\n",
    "    ]\n",
    "    return true_prompt, corrupted_prompts, target"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 23,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 15,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in',\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
       " ['The altiesemale cust acknow confir is located in',\n",
       "  'The INSTabreHEAD stakesiasis is located in',\n",
       "  'The CTV signinggrureditaryllo is located in',\n",
       "  'The desktopuzifforthy Apprentice is located in',\n",
       "  'The kritldom tremendNetMessageemouth is located in'],\n",
<<<<<<< HEAD
       " 'Paris')"
      ]
     },
     "execution_count": 15,
=======
       " ['The tremendenghyde newspmonary is located in',\n",
       "  'The semblyireziffsel Blades is located in',\n",
       "  'The teinhartSPONSOREDirisEngineDebug is located in',\n",
       "  'The [/ Pradeshiffelaordable is located in',\n",
       "  'The sey destrodriversGB pancakes is located in'],\n",
       " 'Paris')"
      ]
     },
     "execution_count": 23,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
       " 'Paris')"
      ]
     },
     "execution_count": 15,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt, subject, target = sample_dataset(dataset)\n",
    "noise_ablation(prompt, subject, target)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 33,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 16,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_from_left(tokens : torch.tensor, maxlen:int):\n",
    "    pad_token = model.tokenizer.pad_token_id\n",
    "    padded_tokenized_inputs = torch.zeros(tokens.shape[0], maxlen)\n",
    "    \n",
    "    n_pads = maxlen - tokens.shape[-1]\n",
    "    padded_tokenized_inputs[:,n_pads] = pad_token\n",
    "    padded_tokenized_inputs[:,n_pads:] = tokens\n",
    "    return padded_tokenized_inputs.long()\n",
    "\n",
    "def pad_to_same_length(clean_tokens, corrupted_tokens): \n",
    "    \n",
    "    maxlen = max([clean_tokens.shape[-1], corrupted_tokens.shape[-1]])\n",
    "    \n",
    "    if clean_tokens.shape[-1] > corrupted_tokens.shape[-1]: \n",
    "        corrupted_tokens = pad_from_left(corrupted_tokens, maxlen)\n",
    "    elif clean_tokens.shape[-1] < corrupted_tokens.shape[-1]: \n",
    "        clean_tokens = pad_from_left(clean_tokens, maxlen)\n",
    "    return clean_tokens, corrupted_tokens"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 98,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 17,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
<<<<<<< HEAD
=======
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False]], device='cuda:0')\n",
      "15\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "16\n",
      "15\n",
      "15\n"
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
<<<<<<< HEAD
       "('The headquarter of  Monell Chemical Senses Center is located in',\n",
       " ['The headquarter of vernacular English has been disabled, is located in',\n",
       "  'The headquarter of urn Enterprises Ltd. in Luty is located in',\n",
       "  'The headquarter of ���마지 is located in',\n",
       "  'The headquarter of iphone 5s has been recalled is located in',\n",
       "  'The headquarter of ileador.com.au has is located in',\n",
       "  'The headquarter of リンビータケー is located in',\n",
       "  'The headquarter of vernacular and urban public schools in is located in'],\n",
       " 'Philadelphia',\n",
       " tensor([False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False, False]))"
      ]
     },
     "execution_count": 17,
=======
       "('The mother tongue of Thomas Joannes Stieltjes is',\n",
       " ['The mother tongue of ordableHEAD toddler intuitivekas jaw haz software Fired is',\n",
       "  'The mother tongue of regained Arlingtonipesuminum fadingperorsavery rubbedjoy is',\n",
       "  'The mother tongue of NPR Entrepreneowment cognography Beats Lines judge specificity is',\n",
       "  'The mother tongue of Desktop tradersdocumented bombings puppy boasting Princeton subtractLife is',\n",
       "  'The mother tongue of abruptazaarochond Wage packingendalengthResultsmanship is',\n",
       "  'The mother tongue of segregationfolk publish Winds summoning vesselsForgeModLoaderoren politically is',\n",
       "  'The mother tongue of package quotesStrong nitria learns prog14 anticipate is'],\n",
       " 'Dutch',\n",
       " tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True, False]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resample_ablation(prompt, subject, target, n_noise_samples=20):\n",
    "    subject = \" \" + subject\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    n_subject_tokens = subject_tokens.shape[-1] - 1\n",
    "    noisy_subject_tokens = []\n",
    "    for i in range(n_subject_tokens):     \n",
    "        embedding = model.W_E\n",
    "        permutations = torch.randperm(embedding.size(0))[:n_noise_samples]\n",
    "        random_samples = embedding[permutations]\n",
    "        random_samples = random_samples.unsqueeze(dim=1)\n",
    "        #we de-embed these rows \n",
    "        random_embeddings = model.unembed(random_samples)\n",
    "        random_tokens = torch.argmax(random_embeddings, dim=-1)\n",
    "        noisy_subject_tokens.append(random_tokens)\n",
    "        \n",
    "    noisy_subject_tokens = torch.stack(noisy_subject_tokens, dim=0)\n",
    "    noisy_subject_tokens = noisy_subject_tokens.transpose(1,0)\n",
    "    \n",
    "    corrupted_facts = []\n",
    "    for random_prompt in range(noisy_subject_tokens.shape[0]): \n",
    "        random_tokens = noisy_subject_tokens[random_prompt]\n",
    "        random_subject = [\n",
    "            model.to_string(t) for t in random_tokens\n",
    "        ]\n",
    "        print(len(random_subject))\n",
    "        random_subject = \" \" + \"\".join(random_subject)\n",
    "        corrupted_facts.append(prompt.format(random_subject.strip()))\n",
    "        \n",
    "        \n",
    "    true_fact = prompt.format(subject.strip())\n",
    "    \n",
    "    fact_tokens = model.to_tokens(true_fact)\n",
    "    subject_mask = torch.zeros_like(fact_tokens, dtype=torch.bool, device=device)\n",
    "\n",
    "    for value in subject_tokens[0, 1:]:\n",
    "        subject_mask |= (fact_tokens == value)    \n",
    "        \n",
    "    \n",
    "    print(subject_mask)\n",
    "    print(len(model.to_str_tokens(true_fact)))\n",
    "    for p in corrupted_facts: \n",
    "        print(len(model.to_str_tokens(p)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    return true_fact, corrupted_facts, target, subject_mask.to(\"cpu\")\n",
    "    \n",
    "    \n",
    "prompt, subject, target = sample_dataset(dataset, idx=1)\n",
    "resample_ablation(prompt, subject, target, n_noise_samples=7)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
       "('The headquarter of  Monell Chemical Senses Center is located in',\n",
       " ['The headquarter of vernacular English has been disabled, is located in',\n",
       "  'The headquarter of urn Enterprises Ltd. in Luty is located in',\n",
       "  'The headquarter of ���마지 is located in',\n",
       "  'The headquarter of iphone 5s has been recalled is located in',\n",
       "  'The headquarter of ileador.com.au has is located in',\n",
       "  'The headquarter of リンビータケー is located in',\n",
       "  'The headquarter of vernacular and urban public schools in is located in'],\n",
       " 'Philadelphia',\n",
       " tensor([False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False, False]))"
      ]
     },
<<<<<<< HEAD
     "execution_count": 182,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
     "execution_count": 17,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask(prompt_tokens, subject_tokens): \n",
    "    A_flattened = prompt_tokens.view(-1)\n",
    "    B_flattened = subject_tokens.view(-1)\n",
    "\n",
    "    mask = torch.zeros_like(A_flattened, dtype=torch.bool)\n",
    "\n",
    "    for i in range(len(A_flattened) - len(B_flattened) + 1):\n",
    "        if torch.all(A_flattened[i:i+len(B_flattened)] == B_flattened):\n",
    "            mask[i:i+len(B_flattened)] = True\n",
    "            \n",
    "    return mask\n",
    "\n",
    "\n",
    "def resample_ablation(prompt, subject, target, n_noise_samples=20, temperature=0.85):\n",
    "    subject = \" \" + subject\n",
    "    subject_tokens = model.to_tokens(subject)[:,1:]\n",
    "    n_subject_tokens = subject_tokens.shape[-1]\n",
    "    prompt_start = prompt.split(\"{\")[0]\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
    "    prompt_end = prompt.split(\"}\")[-1]\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    clean_fact = prompt.format(subject)\n",
    "    \n",
    "    fact_tokens = model.to_tokens(clean_fact)\n",
    "    clean_subject_mask = get_mask(fact_tokens, subject_tokens)\n",
    "\n",
    "\n",
    "    corrupted_facts = []        \n",
    "    while len(corrupted_facts) < n_noise_samples: \n",
    "        generated = model.generate(prompt_start, max_new_tokens=n_subject_tokens,temperature=temperature,verbose=False)\n",
    "        corrupted_subject = generated.split(prompt_start)[-1].strip()\n",
    "        corrupted_fact = prompt.format(corrupted_subject)\n",
    "        corrupted_subject_tokens = model.to_tokens(corrupted_subject)[:,1:]\n",
    "        corrupted_fact_tokens = model.to_tokens(corrupted_fact)\n",
    "        \n",
    "        corrupted_mask = get_mask(prompt_tokens=corrupted_fact_tokens, subject_tokens=corrupted_subject_tokens)\n",
    "        if corrupted_mask.shape != clean_subject_mask.shape: \n",
    "            continue\n",
    "        if all(corrupted_mask==clean_subject_mask):\n",
    "            corrupted_facts.append(corrupted_fact)\n",
    "\n",
    "    return clean_fact, corrupted_facts, target, clean_subject_mask\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "    \n",
    " \n",
    "\n",
    "prompt, subject, target = sample_dataset(dataset, idx=3)\n",
    "resample_ablation(prompt, subject, target, n_noise_samples=7, temperature=0.75)\n",
=======
    "        \n",
    "        \n",
    "        \n",
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    \n",
    " \n",
    "\n",
    "prompt, subject, target = sample_dataset(dataset, idx=3)\n",
<<<<<<< HEAD
    "resample_ablation(prompt, subject, target, n_noise_samples=7)\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "resample_ablation(prompt, subject, target, n_noise_samples=7, temperature=0.75)\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 179,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 18,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_from_left(tokens : torch.tensor, maxlen:int):\n",
    "    pad_token = model.tokenizer.pad_token_id\n",
    "    padded_tokenized_inputs = torch.zeros(tokens.shape[0], maxlen)\n",
    "    \n",
    "    n_pads = maxlen - tokens.shape[-1]\n",
    "    padded_tokenized_inputs[:,n_pads] = pad_token\n",
    "    padded_tokenized_inputs[:,n_pads:] = tokens\n",
    "    return padded_tokenized_inputs.long()\n",
    "\n",
    "def pad_to_same_length(clean_tokens, corrupted_tokens): \n",
    "    \n",
    "    maxlen = max([clean_tokens.shape[-1], corrupted_tokens.shape[-1]])\n",
    "    \n",
    "    if clean_tokens.shape[-1] > corrupted_tokens.shape[-1]: \n",
    "        corrupted_tokens = pad_from_left(corrupted_tokens, maxlen)\n",
    "    elif clean_tokens.shape[-1] < corrupted_tokens.shape[-1]: \n",
    "        clean_tokens = pad_from_left(clean_tokens, maxlen)\n",
    "    return clean_tokens, corrupted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Patch Restoration"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 209,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 19,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt, subject, target = sample_dataset(dataset, idx=0)\n",
    "true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=7)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 252,
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 21,
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unembedding_function(residual_stack, cache) -> float:\n",
    "    #we are only interested in applying the layer norm of the final layer on the final token\n",
    "    #shape: [74, 5, 10, 1280] = n_layers, prompts, tokens, d_model\n",
    "    z = cache.apply_ln_to_stack(residual_stack, layer = -1)\n",
    "    z = z @ model.W_U\n",
    "    return z\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "\n",
    "def get_residual(cache, mle_token_idx, target_token_idx, decomposed=True, return_labels=False): \n",
    "    if decomposed: \n",
    "        retval = cache.decompose_resid(layer=-1, mode=\"all\", return_labels=return_labels)\n",
    "    else: \n",
    "        retval = cache.accumulated_resid(layer=-1, return_labels=return_labels)\n",
    "    if return_labels:\n",
    "        residual, layer_names = retval\n",
    "    else:\n",
    "        residual = retval\n",
    "        \n",
    "    residual = unembedding_function(residual, cache)\n",
    "    #shape: torch.Size([layer, batch, pos, vocab])\n",
    "    residual = residual.permute(0,2,1,3)\n",
    "    \n",
    "    mle_token_idx = mle_token_idx.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    mle_token_idx = mle_token_idx.repeat(residual.shape[0], residual.shape[1], 1,1)\n",
    "    \n",
    "    target_token_idx = target_token_idx.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    target_token_idx = target_token_idx.repeat(residual.shape[0], residual.shape[1],1,1)\n",
    "    \n",
    "    #shape: [layer, tokens, prompts, 1]\n",
    "    mle_residual_logits = (residual.gather(dim=-1, index=mle_token_idx) - residual.mean(dim=-1, keepdim=True))\n",
    "    target_residual_logits = (residual.gather(dim=-1, index=target_token_idx) - residual.mean(dim=-1, keepdim=True))\n",
    "    mle_residual_logits = mle_residual_logits[:,-1,:,:].squeeze()\n",
    "    target_residual_logits = target_residual_logits[:,-1,:,:].squeeze()\n",
    "    \n",
    "    mle_residual_logits = mle_residual_logits.mean(dim=-1)\n",
    "    target_residual_logits = target_residual_logits.mean(dim=-1)\n",
    "    \n",
    "    if return_labels: \n",
    "        return layer_names, mle_residual_logits.to(\"cpu\"), target_residual_logits.to(\"cpu\")\n",
    "    else:\n",
    "        return mle_residual_logits.to(\"cpu\"), target_residual_logits.to(\"cpu\")\n",
    "    \n",
    "    \n",
    "def get_logit_attributions(cache, mle_token_idx, target_token_idx, return_labels=False): \n",
    "    retval = cache.decompose_resid(layer=-1, mode=\"all\", return_labels=return_labels)\n",
    "    if return_labels: \n",
    "        residual_stack, layer_names = retval\n",
    "    else:\n",
    "        residual_stack = retval\n",
    "    \n",
    "    #shape: layer, prompt, pos\n",
    "    mle_logit_attributions = cache.logit_attrs(residual_stack, mle_token_idx)\n",
    "    target_logit_attributions = cache.logit_attrs(residual_stack, target_token_idx)\n",
    "    \n",
    "    mle_logit_attributions = mle_logit_attributions[:,:,-1].mean(dim=-1)\n",
    "    target_logit_attributions = target_logit_attributions[:,:,-1].mean(dim=-1)\n",
    "    if return_labels: \n",
    "        return layer_names, mle_logit_attributions.to(\"cpu\"), target_logit_attributions.to(\"cpu\")\n",
    "    else: \n",
    "        return mle_logit_attributions.to(\"cpu\"), target_logit_attributions.to(\"cpu\")\n",
    "\n",
    "def get_stack_head_results(cache, mle_token_idx, target_token_idx, return_labels=False): \n",
    "    head_results = cache.stack_head_results(layer=-1, return_labels=return_labels)\n",
    "    if return_labels: \n",
    "        head_attributions, layer_names = head_results\n",
    "    else:\n",
    "        head_attributions = head_results\n",
    "        \n",
    "    head_attributions = unembedding_function(head_attributions, cache)\n",
    "    #shape: torch.Size([144, 7, 11, 50257]) = n_layers x n_heads, prompts, tokens, vocab\n",
    "    head_attributions = head_attributions.permute(0,2,1,3)\n",
    "    # torch.Size([144, 11, 7, 1])\n",
    "    head_attributions = head_attributions[:,-1,:,:]\n",
    "    #    torch.Size([144, 7, 50257])\n",
    "    \n",
    "    mle_token_idx = mle_token_idx.unsqueeze(dim=0)\n",
    "    mle_token_idx = mle_token_idx.repeat(head_attributions.shape[0], 1,1)\n",
    "    target_token_idx = target_token_idx.unsqueeze(dim=0)\n",
    "    target_token_idx = target_token_idx.repeat(head_attributions.shape[0],1,1)\n",
    "    \n",
    "    head_attributions_mle = head_attributions.gather(dim=-1, index=target_token_idx) - head_attributions.mean(dim=-1, keepdim=True)\n",
    "    head_attributions_target = head_attributions.gather(dim=-1, index=target_token_idx) - head_attributions.mean(dim=-1, keepdim=True)\n",
    "\n",
    "    head_attributions_mle = head_attributions_mle.squeeze().mean(dim=-1)\n",
    "    head_attributions_target = head_attributions_target.squeeze().mean(dim=-1)\n",
    "\n",
    "    if return_labels: \n",
    "        return layer_names, head_attributions_mle.to(\"cpu\"), head_attributions_target.to(\"cpu\")\n",
    "    else: \n",
    "        return head_attributions_mle.to(\"cpu\"), head_attributions_target.to(\"cpu\")\n",
    "\n",
    "    \n",
=======
    "def get_accumulated_residual(cache, mle_token_idx, target_token_idx): \n",
    "    accumulated_residual = cache.accumulated_resid(layer=-1)\n",
    "    accumulated_residual = unembedding_function(accumulated_residual, cache)\n",
    "    #shape: torch.Size([layer, batch, pos, vocab])\n",
    "    accumulated_residual = accumulated_residual.permute(0,2,1,3)\n",
    "    \n",
    "    print(accumulated_residual.shape)\n",
    "    print(mle_token_idx.shape)\n",
    "    \n",
    "    # mle_token_idx = mle_token_idx.unsqueeze(dim=0).expand(accumulated_residual.shape[1],1,1)\n",
    "    print(mle_token_idx)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "          \n",
    "                                        \n",
    "    raise\n",
    "    \n",
    "    \n",
    "    mle_idx_expanded = mle_token_idx.repeat(accumulated_residual.shape[0],1,accumulated_residual.shape[2],1)\n",
    "    \n",
    "    print(accumulated_residual.shape)\n",
    "    mle_residual_logits = (accumulated_residual.gather(dim=-1, index=mle_idx_expanded) - accumulated_residual.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    target_residual_logits = (accumulated_residual.gather(dim=-1, index=target_idx_expanded) - accumulated_residual.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "\n",
    "def get_residual(cache, mle_token_idx, target_token_idx, decomposed=True, return_labels=False): \n",
    "    if decomposed: \n",
    "        retval = cache.decompose_resid(layer=-1, mode=\"all\", return_labels=return_labels)\n",
    "    else: \n",
    "        retval = cache.accumulated_resid(layer=-1, return_labels=return_labels)\n",
    "    if return_labels:\n",
    "        residual, layer_names = retval\n",
    "    else:\n",
    "        residual = retval\n",
    "        \n",
    "    residual = unembedding_function(residual, cache)\n",
    "    #shape: torch.Size([layer, batch, pos, vocab])\n",
    "    residual = residual.permute(0,2,1,3)\n",
    "    \n",
    "    mle_token_idx = mle_token_idx.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    mle_token_idx = mle_token_idx.repeat(residual.shape[0], residual.shape[1], 1,1)\n",
    "    \n",
    "    target_token_idx = target_token_idx.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    target_token_idx = target_token_idx.repeat(residual.shape[0], residual.shape[1],1,1)\n",
    "    \n",
    "    #shape: [layer, tokens, prompts, 1]\n",
    "    mle_residual_logits = (residual.gather(dim=-1, index=mle_token_idx) - residual.mean(dim=-1, keepdim=True))\n",
    "    target_residual_logits = (residual.gather(dim=-1, index=target_token_idx) - residual.mean(dim=-1, keepdim=True))\n",
    "    mle_residual_logits = mle_residual_logits[:,-1,:,:].squeeze()\n",
    "    target_residual_logits = target_residual_logits[:,-1,:,:].squeeze()\n",
    "    \n",
    "    mle_residual_logits = mle_residual_logits.mean(dim=-1)\n",
    "    target_residual_logits = target_residual_logits.mean(dim=-1)\n",
    "    \n",
    "    if return_labels: \n",
    "        return layer_names, mle_residual_logits.to(\"cpu\"), target_residual_logits.to(\"cpu\")\n",
    "    else:\n",
    "        return mle_residual_logits.to(\"cpu\"), target_residual_logits.to(\"cpu\")\n",
    "    \n",
    "    \n",
    "def get_logit_attributions(cache, mle_token_idx, target_token_idx, return_labels=False): \n",
    "    retval = cache.decompose_resid(layer=-1, mode=\"all\", return_labels=return_labels)\n",
    "    if return_labels: \n",
    "        residual_stack, layer_names = retval\n",
    "    else:\n",
    "        residual_stack = retval\n",
    "    \n",
    "    #shape: layer, prompt, pos\n",
    "    mle_logit_attributions = cache.logit_attrs(residual_stack, mle_token_idx)\n",
    "    target_logit_attributions = cache.logit_attrs(residual_stack, target_token_idx)\n",
    "    \n",
    "    mle_logit_attributions = mle_logit_attributions[:,:,-1].mean(dim=-1)\n",
    "    target_logit_attributions = target_logit_attributions[:,:,-1].mean(dim=-1)\n",
    "    if return_labels: \n",
    "        return layer_names, mle_logit_attributions.to(\"cpu\"), target_logit_attributions.to(\"cpu\")\n",
    "    else: \n",
    "        return mle_logit_attributions.to(\"cpu\"), target_logit_attributions.to(\"cpu\")\n",
    "\n",
<<<<<<< HEAD
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "def get_stack_head_results(cache, mle_token_idx, target_token_idx, return_labels=False): \n",
    "    head_results = cache.stack_head_results(layer=-1, return_labels=return_labels)\n",
    "    if return_labels: \n",
    "        head_attributions, layer_names = head_results\n",
    "    else:\n",
    "        head_attributions = head_results\n",
    "        \n",
    "    head_attributions = unembedding_function(head_attributions, cache)\n",
    "    #shape: torch.Size([144, 7, 11, 50257]) = n_layers x n_heads, prompts, tokens, vocab\n",
    "    head_attributions = head_attributions.permute(0,2,1,3)\n",
    "    # torch.Size([144, 11, 7, 1])\n",
    "    head_attributions = head_attributions[:,-1,:,:]\n",
    "    #    torch.Size([144, 7, 50257])\n",
    "    \n",
    "    mle_token_idx = mle_token_idx.unsqueeze(dim=0)\n",
    "    mle_token_idx = mle_token_idx.repeat(head_attributions.shape[0], 1,1)\n",
    "    target_token_idx = target_token_idx.unsqueeze(dim=0)\n",
    "    target_token_idx = target_token_idx.repeat(head_attributions.shape[0],1,1)\n",
    "    \n",
    "    head_attributions_mle = head_attributions.gather(dim=-1, index=target_token_idx) - head_attributions.mean(dim=-1, keepdim=True)\n",
    "    head_attributions_target = head_attributions.gather(dim=-1, index=target_token_idx) - head_attributions.mean(dim=-1, keepdim=True)\n",
    "\n",
    "    head_attributions_mle = head_attributions_mle.squeeze().mean(dim=-1)\n",
    "    head_attributions_target = head_attributions_target.squeeze().mean(dim=-1)\n",
    "\n",
    "    if return_labels: \n",
    "        return layer_names, head_attributions_mle.to(\"cpu\"), head_attributions_target.to(\"cpu\")\n",
    "    else: \n",
    "        return head_attributions_mle.to(\"cpu\"), head_attributions_target.to(\"cpu\")\n",
    "\n",
    "    \n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "def patch_layer(corrupted_residual_component,hook,cache):\n",
    "    corrupted_residual_component[:, :, :] = cache[hook.name][:, :, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "def patch_position(corrupted_residual_component, hook,pos,cache):\n",
    "    corrupted_residual_component[:, pos, :] = cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "def patch_head_pattern(\n",
    "    corrupted_head_pattern: Float[torch.Tensor, \"batch head_index query_pos d_head\"],\n",
    "    hook, \n",
    "    head_index, \n",
    "    cache):\n",
    "    corrupted_head_pattern[:, head_index, :, :] = cache[hook.name][:, head_index, :, :]\n",
    "    return corrupted_head_pattern\n",
    "\n",
    "\n",
<<<<<<< HEAD
=======
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "def extract_logit(logits, mle_token_idx, target_token_idx):     \n",
    "    mle_logit = (logits.gather(dim=-1, index=mle_token_idx) - logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    target_logit = (logits.gather(dim=-1, index=target_token_idx) - logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    mle_logit = mle_logit.mean(dim=0)\n",
    "    target_logit = target_logit.mean(dim=0)\n",
    "    return mle_logit, target_logit\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "def get_mle_logit(tokens): \n",
    "    reference_logits, reference_cache = model.run_with_cache(tokens, return_type=\"logits\")\n",
    "    mle_token = torch.argmax(reference_logits[:,-1,:], dim=-1)\n",
    "    return mle_token\n",
<<<<<<< HEAD
    "\n",
    "def run_all_single_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True):\n",
=======
    "\n",
    "def run_all_single_patches(\n",
    "                        clean_prompt: str,\n",
    "                        corrupted_prompts: List[str],\n",
    "                        target: str):\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "\n",
    "def run_all_single_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True):\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
=======
    "    n_tokens = clean_tokens.shape[-1]\n",
=======
    "\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
<<<<<<< HEAD
    "    mle_token = torch.argmax(clean_logits[:,-1,:], dim=-1)\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
<<<<<<< HEAD
<<<<<<< HEAD
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers * 3,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers * 3,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers * 3,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers * 3,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers * 3,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers * 3,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers * 3,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers * 3,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers * 3, device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers * 3, device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers * 3):\n",
    "        if layer % 3 == 0: \n",
    "            p = \"resid_pre\"\n",
    "        elif layer % 3 == 1:\n",
    "            p = \"attn_out\"\n",
    "        else: \n",
    "            p = \"mlp_out\"\n",
    "        patch_name = utils.get_act_name(p, layer//3)\n",
    "\n",
    "        hook_fn = partial(patch_layer, cache=ablate_cache)            \n",
    "        with model.hooks(\n",
    "            fwd_hooks = [(patch_name, hook_fn)]\n",
    "        ) as hooked_model:\n",
    "            patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "            #---------------------------calculating patch results---------------------------------------\n",
    "            patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "            patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "            patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "            patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "            patched_logits = patched_logits[:,-1,:]\n",
    "            patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "            \n",
    "            total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "            total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "            total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "            total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "            total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "            total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "            total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "            total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "            total_patched_mle_logit[layer] = patched_mle_logit\n",
    "            total_patched_target_logit[layer] = patched_target_logit\n",
    "            \n",
    "            patched_layer_names.append(patch_name)\n",
    "            \n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "            \n",
    "            raise\n",
    "                \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
=======
    "    clean_logits = clean_logits[:,-1,:]\n",
    "    corrupted_logits = corrupted_logits[:,-1,:]\n",
=======
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers * 3,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers * 3,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers * 3,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers * 3,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers * 3,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers * 3,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers * 3,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers * 3,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers * 3, device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers * 3, device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers * 3):\n",
    "        if layer % 3 == 0: \n",
    "            p = \"resid_pre\"\n",
    "        elif layer % 3 == 1:\n",
    "            p = \"attn_out\"\n",
    "        else: \n",
    "            p = \"mlp_out\"\n",
    "        patch_name = utils.get_act_name(p, layer//3)\n",
    "\n",
    "        hook_fn = partial(patch_layer, cache=ablate_cache)            \n",
    "        with model.hooks(\n",
    "            fwd_hooks = [(patch_name, hook_fn)]\n",
    "        ) as hooked_model:\n",
    "            patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "            #---------------------------calculating patch results---------------------------------------\n",
    "            patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "            patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "            patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "            patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "            patched_logits = patched_logits[:,-1,:]\n",
    "            patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "            \n",
    "            total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "            total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "            total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "            total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "            total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "            total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "            total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "            total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "            total_patched_mle_logit[layer] = patched_mle_logit\n",
    "            total_patched_target_logit[layer] = patched_target_logit\n",
    "            \n",
    "            patched_layer_names.append(patch_name)\n",
    "            \n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "            \n",
    "            raise\n",
    "                \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
<<<<<<< HEAD
    "    }\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2, 3, 4]), array([5, 6, 7, 8, 9]), array([10, 11, 12, 13, 14])]\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False                     #True to run hydra effect, False to run activation patching\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = \"single_patching_restoration\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=True                     #True to run hydra effect, False to run activation patching\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = \"single_patching_hydra\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablate across token positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_position(corrupted_residual_component, hook,pos,cache):\n",
    "    corrupted_residual_component[:, pos, :] = cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "def patch_head_pattern(\n",
    "    corrupted_head_pattern: Float[torch.Tensor, \"batch head_index query_pos d_head\"],\n",
    "    hook, \n",
    "    head_index, \n",
    "    cache):\n",
    "    corrupted_head_pattern[:, head_index, :, :] = cache[hook.name][:, head_index, :, :]\n",
    "    return corrupted_head_pattern\n",
    "\n",
    "\n",
    "def extract_logit(logits, mle_token_idx, target_token_idx):     \n",
    "    mle_logit = (logits.gather(dim=-1, index=mle_token_idx) - logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    target_logit = (logits.gather(dim=-1, index=target_token_idx) - logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    mle_logit = mle_logit.mean(dim=0)\n",
    "    target_logit = target_logit.mean(dim=0)\n",
    "    return mle_logit, target_logit\n",
    "\n",
    "def get_mle_logit(tokens): \n",
    "    reference_logits, reference_cache = model.run_with_cache(tokens, return_type=\"logits\")\n",
    "    mle_token = torch.argmax(reference_logits[:,-1,:], dim=-1)\n",
    "    return mle_token\n",
    "\n",
    "def run_all_single_position_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True):\n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers * 3,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers * 3,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers * 3):\n",
    "        if layer % 3 == 0: \n",
    "            p = \"resid_pre\"\n",
    "        elif layer % 3 == 1:\n",
    "            p = \"attn_out\"\n",
    "        else: \n",
    "            p = \"mlp_out\"\n",
    "        patch_name = utils.get_act_name(p, layer//3)\n",
    "        patched_layer_names.append(patch_name)\n",
    "        \n",
    "        for position in range(n_tokens):\n",
    "            hook_fn = partial(patch_position, pos=position, cache=ablate_cache)\n",
    "            with model.hooks(\n",
    "                fwd_hooks = [(patch_name, hook_fn)]\n",
    "            ) as hooked_model:\n",
    "                patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "                #---------------------------calculating patch results---------------------------------------\n",
    "                patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "                patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "                patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "                patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "                patched_logits = patched_logits[:,-1,:]\n",
    "                patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "                \n",
    "                total_patched_decomposed_residual_mle[layer,position,:] = patched_decomposed_residual_mle\n",
    "                total_patched_decomposed_residual_target[layer,position,:] = patched_decomposed_residual_target\n",
    "                total_patched_accumulated_residual_mle[layer,position,:] = patched_accumulated_residual_mle\n",
    "                total_patched_accumulated_residual_target[layer,position,:] = patched_accumulated_residual_target\n",
    "                total_patched_logit_attr_mle[layer,position,:] = patched_logit_attr_mle\n",
    "                total_patched_logit_attr_target[layer,position,:] = patched_logit_attr_target\n",
    "                total_patched_head_attributions_mle[layer,position,:] = patched_head_attributions_mle\n",
    "                total_patched_head_attributions_target[layer,position,:] = patched_head_attributions_target\n",
    "                total_patched_mle_logit[layer,position] = patched_mle_logit\n",
    "                total_patched_target_logit[layer, position] = patched_target_logit\n",
    "                \n",
    "                \n",
    "                hooked_model.reset_hooks()\n",
    "                model.reset_hooks()\n",
    "                            \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_position_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=True                     #True to run hydra effect, False to run activation patching\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = \"position_patching_hydra\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
=======
   "execution_count": 253,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/690718292.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = run_all_single_position_patches(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         ) \n",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/3707154454.py\u001b[0m in \u001b[0;36mrun_all_single_position_patches\u001b[0;34m(clean_prompt, corrupted_prompts, target, ablate_with_corrupted)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return {\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
=======
      "\u001b[0;32m/tmp/ipykernel_25797/4053907738.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = run_all_single_patches(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         ) \n",
      "\u001b[0;32m/tmp/ipykernel_25797/3978651084.py\u001b[0m in \u001b[0;36mrun_all_single_patches\u001b[0;34m(clean_prompt, corrupted_prompts, target)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtarget_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_single_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mclean_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mcorrupted_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupted_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHookedRootModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[0;32m--> 437\u001b[0;31m         out, cache_dict = super().run_with_cache(\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         )\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mclear_contexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_contexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ):\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincl_bwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 )\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             residual = block(\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mpast_kv_cache_entry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         q = self.hook_q(\n\u001b[0;32m--> 508\u001b[0;31m             einsum(\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mqkv_einops_string\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_model\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0;34m->\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mpos\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mnew_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_equation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_equation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2, 3, 4]), array([5, 6, 7, 8, 9]), array([10, 11, 12, 13, 14])]\n"
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_position_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False                     #True to run hydra effect, False to run activation patching\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = \"position_patching_restoration\"\n",
    "              \n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Window Patch"
=======
    "\n",
    "result = run_all_single_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "        ) "
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "generate_sliding_windows(5, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def run_all_single_window_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True, \n",
    "        window_size=2,\n",
    "        step_size=1, \n",
    "        ablate=[\"attn_out\", \"mlp_out\"]):\n",
    "    for layer_type in ablate:\n",
    "        assert layer_type in [\"attn_out\", \"mlp_out\"]\n",
    "        \n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers, device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers, device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    sliding_windows = generate_sliding_windows(n_tokens, window_size, step_size)\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        window = sliding_windows[layer]\n",
    "        patches = []\n",
    "        for l in window: \n",
    "            for layer_type in ablate: \n",
    "                patches.append(utils.get_act_name(layer_type, l))\n",
    "        patched_layer_names.append(patches)\n",
    "\n",
    "        hook_fn = partial(patch_layer, cache=ablate_cache)            \n",
    "        fwd_hooks = [(patch, hook_fn) for patch in patches]\n",
    "        with model.hooks(\n",
    "            fwd_hooks = fwd_hooks\n",
    "        ) as hooked_model:\n",
    "            patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "            #---------------------------calculating patch results---------------------------------------\n",
    "            patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "            patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "            patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "            patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "            patched_logits = patched_logits[:,-1,:]\n",
    "            patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "            \n",
    "            total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "            total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "            total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "            total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "            total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "            total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "            total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "            total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "            total_patched_mle_logit[layer] = patched_mle_logit\n",
    "            total_patched_target_logit[layer] = patched_target_logit\n",
    "            \n",
    "            \n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "                            \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\", \"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"single_window_ws{window_size}_ss{step_size}_restoration_all_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"single_window_ws{window_size}_ss{step_size}_restoration_mlp_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
=======
   "execution_count": 254,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/3886324668.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = run_all_single_window_patches(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         ) \n",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/1226908749.py\u001b[0m in \u001b[0;36mrun_all_single_window_patches\u001b[0;34m(clean_prompt, corrupted_prompts, target, ablate_with_corrupted, window_size, step_size, ablate)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     return {\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
=======
      "\u001b[0;32m/tmp/ipykernel_25797/1928563440.py\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"results/{indices.min()}-{indices.max()}_restoration_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25797/1928563440.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_ablation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_noise_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         result = run_all_restoration(\n",
      "\u001b[0;32m/tmp/ipykernel_25797/2627377753.py\u001b[0m in \u001b[0;36mresample_ablation\u001b[0;34m(prompt, subject, target, n_noise_samples, temperature)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcorrupted_facts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_noise_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_subject_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mcorrupted_subject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcorrupted_fact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupted_subject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, num_return_sequences, use_past_kv_cache, prepend_bos, return_type, verbose)\u001b[0m\n\u001b[1;32m   1530\u001b[0m                     )\n\u001b[1;32m   1531\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m                     logits = self.forward(\n\u001b[0m\u001b[1;32m   1533\u001b[0m                         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m                         \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 )\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             residual = block(\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mpast_kv_cache_entry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         q = self.hook_q(\n\u001b[0;32m--> 508\u001b[0;31m             einsum(\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mqkv_einops_string\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_model\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0;34m->\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mpos\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mnew_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_equation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_equation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
=======
    "def run_experiment(indices): \n",
    "    # indices = list(range(len(dataset)))\n",
    "    # random.shuffle(indices)\n",
    "    # indices = indices[:n]\n",
    "    \n",
    "    results = []\n",
    "    \n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "def run_experiment(indices):     \n",
    "    results = []\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        result = run_all_single_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\"]\n",
=======
    "        result = run_all_restoration(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "        result = run_all_single_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False                     #True to run hydra effect, False to run activation patching\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"single_window_ws{window_size}_ss{step_size}_restoration_attn_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Window Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def run_all_positional_window_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True, \n",
    "        window_size=2,\n",
    "        step_size=1, \n",
    "        ablate=[\"attn_out\", \"mlp_out\"]):\n",
    "    for layer_type in ablate:\n",
    "        assert layer_type in [\"attn_out\", \"mlp_out\"]\n",
    "        \n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers,n_tokens, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers,n_tokens,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers,n_tokens,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers,n_tokens,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    sliding_windows = generate_sliding_windows(n_tokens, window_size, step_size)\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        window = sliding_windows[layer]\n",
    "        patches = []\n",
    "        for l in window: \n",
    "            for layer_type in ablate: \n",
    "                patches.append(utils.get_act_name(layer_type, l))\n",
    "        patched_layer_names.append(patches)\n",
    "\n",
    "        for position in range(n_tokens):\n",
    "            hook_fn = partial(patch_position, pos=position, cache=ablate_cache)            \n",
    "            fwd_hooks = [(patch, hook_fn) for patch in patches]\n",
    "            with model.hooks(\n",
    "                fwd_hooks = fwd_hooks\n",
    "            ) as hooked_model:\n",
    "\n",
    "                patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "                #---------------------------calculating patch results---------------------------------------\n",
    "                patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "                patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "                patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "                patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "                patched_logits = patched_logits[:,-1,:]\n",
    "                patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "                \n",
    "                total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "                total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "                total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "                total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "                total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "                total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "                total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "                total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "                total_patched_mle_logit[layer] = patched_mle_logit\n",
    "                total_patched_target_logit[layer] = patched_target_logit\n",
    "                \n",
    "                \n",
    "                hooked_model.reset_hooks()\n",
    "                model.reset_hooks()\n",
    "                \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/3387557432.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = run_all_positional_window_patches(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         ) \n",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/1755867272.py\u001b[0m in \u001b[0;36mrun_all_positional_window_patches\u001b[0;34m(clean_prompt, corrupted_prompts, target, ablate_with_corrupted, window_size, step_size, ablate)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     return {\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\",\"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"positional_window_ws{window_size}_ss{step_size}_restoration_all_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
=======
    "index_batches = [np.arange(i, 100+i) for i in [200]]\n",
    "\n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_restoration_results.pickle\"\n",
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = \"single_patching_restoration\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
<<<<<<< HEAD
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
=======
   "source": [
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
<<<<<<< HEAD
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"mlp_out\"]\n",
=======
    "        result = run_all_single_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=True                     #True to run hydra effect, False to run activation patching\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
<<<<<<< HEAD
    "experiment_name = f\"positional_window_ws{window_size}_ss{step_size}_restoration_mlp_act\"\n",
=======
    "experiment_name = \"single_patching_hydra\"\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
<<<<<<< HEAD
=======
   "source": []
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablate across token positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_position(corrupted_residual_component, hook,pos,cache):\n",
    "    corrupted_residual_component[:, pos, :] = cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "def patch_head_pattern(\n",
    "    corrupted_head_pattern: Float[torch.Tensor, \"batch head_index query_pos d_head\"],\n",
    "    hook, \n",
    "    head_index, \n",
    "    cache):\n",
    "    corrupted_head_pattern[:, head_index, :, :] = cache[hook.name][:, head_index, :, :]\n",
    "    return corrupted_head_pattern\n",
    "\n",
    "\n",
    "def extract_logit(logits, mle_token_idx, target_token_idx):     \n",
    "    mle_logit = (logits.gather(dim=-1, index=mle_token_idx) - logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    target_logit = (logits.gather(dim=-1, index=target_token_idx) - logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    mle_logit = mle_logit.mean(dim=0)\n",
    "    target_logit = target_logit.mean(dim=0)\n",
    "    return mle_logit, target_logit\n",
    "\n",
    "def get_mle_logit(tokens): \n",
    "    reference_logits, reference_cache = model.run_with_cache(tokens, return_type=\"logits\")\n",
    "    mle_token = torch.argmax(reference_logits[:,-1,:], dim=-1)\n",
    "    return mle_token\n",
    "\n",
    "def run_all_single_position_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True):\n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers * 3,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers * 3,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers * 3,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers * 3):\n",
    "        if layer % 3 == 0: \n",
    "            p = \"resid_pre\"\n",
    "        elif layer % 3 == 1:\n",
    "            p = \"attn_out\"\n",
    "        else: \n",
    "            p = \"mlp_out\"\n",
    "        patch_name = utils.get_act_name(p, layer//3)\n",
    "        patched_layer_names.append(patch_name)\n",
    "        \n",
    "        for position in range(n_tokens):\n",
    "            hook_fn = partial(patch_position, pos=position, cache=ablate_cache)\n",
    "            with model.hooks(\n",
    "                fwd_hooks = [(patch_name, hook_fn)]\n",
    "            ) as hooked_model:\n",
    "                patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "                #---------------------------calculating patch results---------------------------------------\n",
    "                patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "                patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "                patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "                patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "                patched_logits = patched_logits[:,-1,:]\n",
    "                patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "                \n",
    "                total_patched_decomposed_residual_mle[layer,position,:] = patched_decomposed_residual_mle\n",
    "                total_patched_decomposed_residual_target[layer,position,:] = patched_decomposed_residual_target\n",
    "                total_patched_accumulated_residual_mle[layer,position,:] = patched_accumulated_residual_mle\n",
    "                total_patched_accumulated_residual_target[layer,position,:] = patched_accumulated_residual_target\n",
    "                total_patched_logit_attr_mle[layer,position,:] = patched_logit_attr_mle\n",
    "                total_patched_logit_attr_target[layer,position,:] = patched_logit_attr_target\n",
    "                total_patched_head_attributions_mle[layer,position,:] = patched_head_attributions_mle\n",
    "                total_patched_head_attributions_target[layer,position,:] = patched_head_attributions_target\n",
    "                total_patched_mle_logit[layer,position] = patched_mle_logit\n",
    "                total_patched_target_logit[layer, position] = patched_target_logit\n",
    "                \n",
    "                \n",
    "                hooked_model.reset_hooks()\n",
    "                model.reset_hooks()\n",
    "                            \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
<<<<<<< HEAD
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\"]\n",
=======
    "        result = run_all_single_position_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=True                     #True to run hydra effect, False to run activation patching\n",
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"positional_window_ws{window_size}_ss{step_size}_restoration_attn_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablating subject positions across windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def run_all_subject_positional_window_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        subject_mask : torch.Tensor,\n",
    "        ablate_with_corrupted=True, \n",
    "        window_size=2,\n",
    "        step_size=1, \n",
    "        ablate=[\"attn_out\", \"mlp_out\"], ):\n",
    "    \n",
    "    subject_positions = subject_mask.nonzero().squeeze()\n",
    "    \n",
    "    for layer_type in ablate:\n",
    "        assert layer_type in [\"attn_out\", \"mlp_out\"]\n",
    "        \n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers,device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers,device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    sliding_windows = generate_sliding_windows(n_tokens, window_size, step_size)\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        window = sliding_windows[layer]\n",
    "        patches = []\n",
    "        for l in window: \n",
    "            for layer_type in ablate: \n",
    "                patches.append(utils.get_act_name(layer_type, l))\n",
    "        patched_layer_names.append(patches)\n",
    "\n",
    "        fwd_hooks = []\n",
    "        for position in subject_positions:\n",
    "            hook_fn = partial(patch_position, pos=position, cache=ablate_cache)            \n",
    "            fwd_hooks += [(patch, hook_fn) for patch in patches]\n",
    "                        \n",
    "        with model.hooks(\n",
    "            fwd_hooks = fwd_hooks\n",
    "        ) as hooked_model:\n",
    "\n",
    "            patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "            #---------------------------calculating patch results---------------------------------------\n",
    "            patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "            patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "            patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "            patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "            patched_logits = patched_logits[:,-1,:]\n",
    "            patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "            \n",
    "            total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "            total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "            total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "            total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "            total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "            total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "            total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "            total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "            total_patched_mle_logit[layer] = patched_mle_logit\n",
    "            total_patched_target_logit[layer] = patched_target_logit\n",
    "            \n",
    "            \n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "                \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            subject_mask=subject_mask,\n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\", \"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"subject_window_ws{window_size}_ss{step_size}_restoration_all_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
=======
    "    \n",
=======
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = \"position_patching_hydra\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/690718292.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = run_all_single_position_patches(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         ) \n",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/3707154454.py\u001b[0m in \u001b[0;36mrun_all_single_position_patches\u001b[0;34m(clean_prompt, corrupted_prompts, target, ablate_with_corrupted)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return {\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_position_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False                     #True to run hydra effect, False to run activation patching\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = \"position_patching_restoration\"\n",
    "              \n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Window Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "generate_sliding_windows(5, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def run_all_single_window_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True, \n",
    "        window_size=2,\n",
    "        step_size=1, \n",
    "        ablate=[\"attn_out\", \"mlp_out\"]):\n",
    "    for layer_type in ablate:\n",
    "        assert layer_type in [\"attn_out\", \"mlp_out\"]\n",
    "        \n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
<<<<<<< HEAD
    "    if return_labels: \n",
    "        return labels, mle_residual_logits, target_residual_logits\n",
    "    return mle_residual_logits, target_residual_logits\n"
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers, device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers, device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    sliding_windows = generate_sliding_windows(n_tokens, window_size, step_size)\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        window = sliding_windows[layer]\n",
    "        patches = []\n",
    "        for l in window: \n",
    "            for layer_type in ablate: \n",
    "                patches.append(utils.get_act_name(layer_type, l))\n",
    "        patched_layer_names.append(patches)\n",
    "\n",
    "        hook_fn = partial(patch_layer, cache=ablate_cache)            \n",
    "        fwd_hooks = [(patch, hook_fn) for patch in patches]\n",
    "        with model.hooks(\n",
    "            fwd_hooks = fwd_hooks\n",
    "        ) as hooked_model:\n",
    "            patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "            #---------------------------calculating patch results---------------------------------------\n",
    "            patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "            patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "            patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "            patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "            patched_logits = patched_logits[:,-1,:]\n",
    "            patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "            \n",
    "            total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "            total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "            total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "            total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "            total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "            total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "            total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "            total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "            total_patched_mle_logit[layer] = patched_mle_logit\n",
    "            total_patched_target_logit[layer] = patched_target_logit\n",
    "            \n",
    "            \n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "                            \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\", \"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"single_window_ws{window_size}_ss{step_size}_restoration_all_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"single_window_ws{window_size}_ss{step_size}_restoration_mlp_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/3886324668.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = run_all_single_window_patches(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         ) \n",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/1226908749.py\u001b[0m in \u001b[0;36mrun_all_single_window_patches\u001b[0;34m(clean_prompt, corrupted_prompts, target, ablate_with_corrupted, window_size, step_size, ablate)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     return {\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_single_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"single_window_ws{window_size}_ss{step_size}_restoration_attn_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Window Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def run_all_positional_window_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        ablate_with_corrupted=True, \n",
    "        window_size=2,\n",
    "        step_size=1, \n",
    "        ablate=[\"attn_out\", \"mlp_out\"]):\n",
    "    for layer_type in ablate:\n",
    "        assert layer_type in [\"attn_out\", \"mlp_out\"]\n",
    "        \n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers,n_tokens, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers,n_tokens,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers,n_tokens,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers,n_tokens,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers,n_tokens,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers,n_tokens,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers,n_tokens,device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    sliding_windows = generate_sliding_windows(n_tokens, window_size, step_size)\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        window = sliding_windows[layer]\n",
    "        patches = []\n",
    "        for l in window: \n",
    "            for layer_type in ablate: \n",
    "                patches.append(utils.get_act_name(layer_type, l))\n",
    "        patched_layer_names.append(patches)\n",
    "\n",
    "        for position in range(n_tokens):\n",
    "            hook_fn = partial(patch_position, pos=position, cache=ablate_cache)            \n",
    "            fwd_hooks = [(patch, hook_fn) for patch in patches]\n",
    "            with model.hooks(\n",
    "                fwd_hooks = fwd_hooks\n",
    "            ) as hooked_model:\n",
    "\n",
    "                patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "                #---------------------------calculating patch results---------------------------------------\n",
    "                patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "                patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "                patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "                patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "                patched_logits = patched_logits[:,-1,:]\n",
    "                patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "                \n",
    "                total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "                total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "                total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "                total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "                total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "                total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "                total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "                total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "                total_patched_mle_logit[layer] = patched_mle_logit\n",
    "                total_patched_target_logit[layer] = patched_target_logit\n",
    "                \n",
    "                \n",
    "                hooked_model.reset_hooks()\n",
    "                model.reset_hooks()\n",
    "                \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/3387557432.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = run_all_positional_window_patches(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         ) \n",
      "\u001b[0;32m/var/folders/xw/wbth19cj4pj2bjqf4kf29jpm0000gn/T/ipykernel_9012/1755867272.py\u001b[0m in \u001b[0;36mrun_all_positional_window_patches\u001b[0;34m(clean_prompt, corrupted_prompts, target, ablate_with_corrupted, window_size, step_size, ablate)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     return {\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\",\"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"positional_window_ws{window_size}_ss{step_size}_restoration_all_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"positional_window_ws{window_size}_ss{step_size}_restoration_mlp_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"positional_window_ws{window_size}_ss{step_size}_restoration_attn_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablating subject positions across windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def run_all_subject_positional_window_patches(\n",
    "        clean_prompt: str,\n",
    "        corrupted_prompts: List[str],\n",
    "        target: str, \n",
    "        subject_mask : torch.Tensor,\n",
    "        ablate_with_corrupted=True, \n",
    "        window_size=2,\n",
    "        step_size=1, \n",
    "        ablate=[\"attn_out\", \"mlp_out\"], ):\n",
    "    \n",
    "    subject_positions = subject_mask.nonzero().squeeze()\n",
    "    \n",
    "    for layer_type in ablate:\n",
    "        assert layer_type in [\"attn_out\", \"mlp_out\"]\n",
    "        \n",
    "    #-----------------------------prepare inputs--------------------------------------\n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    assert clean_tokens.shape[-1] == corrupted_tokens.shape[-1]\n",
    "   \n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token = model.to_single_token(target)\n",
    "    \n",
    "    if ablate_with_corrupted:\n",
    "        ablate_tokens = corrupted_tokens\n",
    "        reference_tokens = clean_tokens\n",
    "    else:\n",
    "        ablate_tokens = clean_tokens\n",
    "        reference_tokens = corrupted_tokens\n",
    "    \n",
    "    reference_logits, reference_cache = model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "    ablate_logits, ablate_cache = model.run_with_cache(ablate_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token = get_mle_logit(clean_tokens)        \n",
    "    target_token = torch.ones_like(mle_token).long().to(device) * target_token\n",
    "    target_token = target_token.unsqueeze(dim=-1)\n",
    "    mle_token = mle_token.unsqueeze(dim=-1)\n",
    "    \n",
    "    reference_logits = reference_logits[:,-1,:]\n",
    "    ablate_logits = ablate_logits[:,-1,:]\n",
    "    \n",
    "    reference_mle_logit, reference_target_logit = extract_logit(reference_logits, mle_token, target_token)\n",
    "    ablate_mle_logit, ablate_target_logit = extract_logit(ablate_logits, mle_token, target_token)\n",
    "    \n",
    "    #---------------------------calculating base results---------------------------------------\n",
    "    layer_names, ref_decomposed_residual_mle, ref_decomposed_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=True, return_labels=True)\n",
    "    ablate_decomposed_residual_mle, ablate_decomposed_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=True)\n",
    "    \n",
    "    resid_names, ref_accumulated_residual_mle, ref_accumulated_residual_target = get_residual(reference_cache, mle_token, target_token, decomposed=False, return_labels=True)\n",
    "    ablate_accumulated_residual_mle, ablate_accumulated_residual_target = get_residual(ablate_cache, mle_token, target_token, decomposed=False)\n",
    "    \n",
    "    ref_logit_attr_mle, ref_logit_attr_target = get_logit_attributions(reference_cache, mle_token, target_token, return_labels=False)\n",
    "    ablate_logit_attr_mle, ablate_logit_attr_target = get_logit_attributions(ablate_cache, mle_token, target_token)\n",
    "    \n",
    "    head_names, ref_head_attributions_mle, ref_head_attributions_target = get_stack_head_results(reference_cache, mle_token, target_token, return_labels=True)\n",
    "    ablate_head_attributions_mle, ablate_head_attributions_target = get_stack_head_results(ablate_cache, mle_token, target_token)\n",
    "\n",
    "    #---------------------------calculating patch results---------------------------------------\n",
    "    total_patched_decomposed_residual_mle = torch.zeros(model.cfg.n_layers, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_decomposed_residual_target = torch.zeros(model.cfg.n_layers,len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_mle = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_accumulated_residual_target = torch.zeros(model.cfg.n_layers,len(resid_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_mle = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_mle), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_logit_attr_target = torch.zeros(model.cfg.n_layers,len(ref_logit_attr_target), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_mle = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_head_attributions_target = torch.zeros(model.cfg.n_layers,len(head_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_mle_logit = torch.zeros(model.cfg.n_layers,device=\"cpu\", dtype=torch.float32)\n",
    "    total_patched_target_logit = torch.zeros(model.cfg.n_layers,device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    sliding_windows = generate_sliding_windows(n_tokens, window_size, step_size)\n",
    "    patched_layer_names = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        window = sliding_windows[layer]\n",
    "        patches = []\n",
    "        for l in window: \n",
    "            for layer_type in ablate: \n",
    "                patches.append(utils.get_act_name(layer_type, l))\n",
    "        patched_layer_names.append(patches)\n",
    "\n",
    "        fwd_hooks = []\n",
    "        for position in subject_positions:\n",
    "            hook_fn = partial(patch_position, pos=position, cache=ablate_cache)            \n",
    "            fwd_hooks += [(patch, hook_fn) for patch in patches]\n",
    "                        \n",
    "        with model.hooks(\n",
    "            fwd_hooks = fwd_hooks\n",
    "        ) as hooked_model:\n",
    "\n",
    "            patched_logits, patched_cache = hooked_model.run_with_cache(reference_tokens, return_type=\"logits\")\n",
    "            #---------------------------calculating patch results---------------------------------------\n",
    "            patched_decomposed_residual_mle, patched_decomposed_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=True)\n",
    "            patched_accumulated_residual_mle, patched_accumulated_residual_target = get_residual(patched_cache, mle_token, target_token, decomposed=False)\n",
    "            patched_logit_attr_mle, patched_logit_attr_target = get_logit_attributions(patched_cache, mle_token, target_token)\n",
    "            patched_head_attributions_mle, patched_head_attributions_target = get_stack_head_results(patched_cache, mle_token, target_token)\n",
    "\n",
    "            patched_logits = patched_logits[:,-1,:]\n",
    "            patched_mle_logit, patched_target_logit = extract_logit(patched_logits, mle_token, target_token)\n",
    "            \n",
    "            total_patched_decomposed_residual_mle[layer,:] = patched_decomposed_residual_mle\n",
    "            total_patched_decomposed_residual_target[layer,:] = patched_decomposed_residual_target\n",
    "            total_patched_accumulated_residual_mle[layer,:] = patched_accumulated_residual_mle\n",
    "            total_patched_accumulated_residual_target[layer,:] = patched_accumulated_residual_target\n",
    "            total_patched_logit_attr_mle[layer,:] = patched_logit_attr_mle\n",
    "            total_patched_logit_attr_target[layer,:] = patched_logit_attr_target\n",
    "            total_patched_head_attributions_mle[layer,:] = patched_head_attributions_mle\n",
    "            total_patched_head_attributions_target[layer,:] = patched_head_attributions_target\n",
    "            total_patched_mle_logit[layer] = patched_mle_logit\n",
    "            total_patched_target_logit[layer] = patched_target_logit\n",
    "            \n",
    "            \n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "                \n",
    "    return {\n",
    "    \"patched_layer_names\" : patched_layer_names,\n",
    "    \"layer_names\" : layer_names,   \n",
    "    \"resid_names\" : resid_names, \n",
    "    \"head_names\" : head_names,\n",
    "    \n",
    "    \"reference_mle_logit\" : reference_mle_logit,\n",
    "    \"reference_target_logit\":reference_target_logit,\n",
    "    \"ablate_mle_logit\":ablate_mle_logit,\n",
    "    \"ablate_target_logit\":ablate_target_logit,\n",
    "    \n",
    "    \"reference_decomposed_residual_mle\":ref_decomposed_residual_mle,\n",
    "    \"reference_decomposed_residual_target\":ref_decomposed_residual_target,\n",
    "    \"ablate_decomposed_residual_mle\":ablate_decomposed_residual_mle,\n",
    "    \"ablate_decomposed_residual_target\":ablate_decomposed_residual_target,\n",
    "    \n",
    "    \"reference_accumulated_residual_mle\":ref_accumulated_residual_mle,\n",
    "    \"reference_accumulated_residual_target\":ref_accumulated_residual_target,\n",
    "    \"ablate_accumulated_residual_mle\":ablate_accumulated_residual_mle,\n",
    "    \"ablate_accumulated_residual_target\":ablate_accumulated_residual_target,\n",
    "    \n",
    "    \"reference_logit_attr_mle\":ref_logit_attr_mle,\n",
    "    \"reference_logit_attr_target\":ref_logit_attr_target,\n",
    "    \"ablate_logit_attr_mle\":ablate_logit_attr_mle,\n",
    "    \"ablate_logit_attr_target\":ablate_logit_attr_target,\n",
    "    \n",
    "    \"reference_head_attributions_mle\":ref_head_attributions_mle,\n",
    "    \"reference_head_attributions_target\":ref_head_attributions_target,\n",
    "    \"ablate_head_attributions_mle\":ablate_head_attributions_mle,\n",
    "    \"ablate_head_attributions_target\":ablate_head_attributions_target,\n",
    "\n",
    "    \"total_patched_decomposed_residual_mle\" : total_patched_decomposed_residual_mle,\n",
    "    \"total_patched_decomposed_residual_target\" : total_patched_decomposed_residual_target,\n",
    "    \"total_patched_accumulated_residual_mle\" : total_patched_accumulated_residual_mle,\n",
    "    \"total_patched_accumulated_residual_target\" : total_patched_accumulated_residual_target, \n",
    "    \"total_patched_logit_attr_mle\" : total_patched_logit_attr_mle, \n",
    "    \"total_patched_logit_attr_target\" : total_patched_logit_attr_target,\n",
    "    \"total_patched_head_attributions_mle\" : total_patched_head_attributions_mle,\n",
    "    \"total_patched_head_attributions_target\" : total_patched_head_attributions_target,\n",
    "    \"total_patched_mle_logit\" : total_patched_mle_logit,\n",
    "    \"total_patched_target_logit\" : total_patched_target_logit, \n",
    "        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "step_size = 2\n",
    "\n",
    "def run_experiment(indices):     \n",
    "    results = []\n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all_subject_positional_window_patches(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            subject_mask=subject_mask,\n",
    "            ablate_with_corrupted=False,                    #True to run hydra effect, False to run activation patching\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            ablate=[\"attn_out\", \"mlp_out\"]\n",
    "        )\n",
    "        result[\"prompt\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "n_experiments = 3\n",
    "experiment_size = 100\n",
    "index_batches = [np.arange(i*experiment_size, experiment_size*i+experiment_size) for i in range(n_experiments)]\n",
    "experiment_name = f\"subject_window_ws{window_size}_ss{step_size}_restoration_all_act\"\n",
    "                 \n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_{experiment_name}_results.pickle\"\n",
    "    results = run_experiment(indices)\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n"
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
<<<<<<< HEAD
<<<<<<< HEAD
   "display_name": "transformer-circuits",
   "language": "python",
   "name": "python3"
=======
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "display_name": "transformer-circuits",
   "language": "python",
   "name": "python3"
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
<<<<<<< HEAD
   "version": "3.10.11"
=======
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb812820b5094695c8a581672e17220e30dd2c15d704c018326e3cc2e1a566f1"
   }
>>>>>>> dc8ca136e5b35632320a750c4c63019f2dd3886a
=======
   "version": "3.10.11"
>>>>>>> b48a1984640c2d9446b21fd3a61902f644833c0f
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03055d1e38844792bbd9750b2daf786a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0336fd949a66477c9daaca7b78b5a3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_669e15f191394c0ca74630c025b164bc",
       "IPY_MODEL_f32ea620bbc342dab3179e632f8fbe44",
       "IPY_MODEL_d276c9530d9f456e9ae9c0b3a931cb93"
      ],
      "layout": "IPY_MODEL_ea6138d3d54f483587595bdda5211215"
     }
    },
    "05c094717d594d37985f58c51527db59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0632336b81a043d09cab7a8b3774968c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f3cef4147924e129b11161067776df0",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03055d1e38844792bbd9750b2daf786a",
      "value": 124
     }
    },
    "08b2a881d8b44bca9eabbe25558d2b10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f3cef4147924e129b11161067776df0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1198949157d045b4b1d56330c21f861b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13e8123052ad4599829fc081dac4eab3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53ab2b48537d45f0a311d51bdf2f03ff",
      "placeholder": "​",
      "style": "IPY_MODEL_f36e03e930584b7d821b4184c54a5810",
      "value": " 456k/456k [00:00&lt;00:00, 9.60MB/s]"
     }
    },
    "1c3f06e311c648569f17cf18f9c2f4f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95713e453b7945d4bf3980f1ae512033",
      "placeholder": "​",
      "style": "IPY_MODEL_d767ae133f1a47bab5a40f4d8ac0d272",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "1ff7229e899e4afab9b5ee0b1597f5e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "267fca47895e493e82c9ef0568847b18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "272adc9fc3ba4c07943725eb800a18c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2df76b7b518948d88d9aba8f670e7f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30e53c8500a14fb880c2ab52007d1087": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32421abdc8fa41d8917203f6bfad586d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "340a9c92995a4189b69e459b033ae946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3419184787645f9b617dc18ccaf315a",
      "placeholder": "​",
      "style": "IPY_MODEL_45df25e66c6041d6b0470865f43d3030",
      "value": " 665/665 [00:00&lt;00:00, 27.5kB/s]"
     }
    },
    "4193f94efd9e457483fc4a2b03a3c07d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45df25e66c6041d6b0470865f43d3030": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a08436dc4c54a1ab2219e66058f68b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4baa43529767484fb04fb1f9e94c1aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4dd5d713b82742df9b120193752b769c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50fd7bca1a95484fa13c37242a988fc3",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6309b4c1e92f46e9b09f9615d20705d8",
      "value": 665
     }
    },
    "50fd7bca1a95484fa13c37242a988fc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ab2b48537d45f0a311d51bdf2f03ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5553fd97122a493fb99e3af859d16ddf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a08436dc4c54a1ab2219e66058f68b5",
      "placeholder": "​",
      "style": "IPY_MODEL_e75523e3b32a449bb5f08ab49d57a473",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "576faf57e7234808866d8ae0c273ecbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57a96c26b3944e7486bb2ae8c26d8973": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e5075a3f50b34526bdd4e215488341af",
       "IPY_MODEL_0632336b81a043d09cab7a8b3774968c",
       "IPY_MODEL_b5defcb6637e4847a76bb982bd40dba4"
      ],
      "layout": "IPY_MODEL_c21ba829e49b4824a83d9e79ec5b9892"
     }
    },
    "5f7b0750af3646b787026494a1872a1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6208163737e24a8ebc66fa95c7f4217f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6309b4c1e92f46e9b09f9615d20705d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6586f332f4104d8d914b8d390049310b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "669e15f191394c0ca74630c025b164bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1d36249ca9b430dad8d8327138b8389",
      "placeholder": "​",
      "style": "IPY_MODEL_a865c5a49b0f4e84a6621b16d682edbd",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "7413bf18522241d3807a01f016d6139e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e9d95a57d194b379f33467895fc8883": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7eecaf2e1f6d401b9d85abf1438cde9b",
       "IPY_MODEL_981666d6021845389f9fa1a0ff5c7997",
       "IPY_MODEL_d54bba9da6d54cbd8e83eec29dae02c4"
      ],
      "layout": "IPY_MODEL_2df76b7b518948d88d9aba8f670e7f9c"
     }
    },
    "7eecaf2e1f6d401b9d85abf1438cde9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2ce3a86d5b34b2eb263fedcbbce72a0",
      "placeholder": "​",
      "style": "IPY_MODEL_4baa43529767484fb04fb1f9e94c1aab",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "80bce29dfc73463c9b4cbeaea636ed74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e240513c73c2418f835465c3a93683f2",
      "placeholder": "​",
      "style": "IPY_MODEL_576faf57e7234808866d8ae0c273ecbd",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 16.1MB/s]"
     }
    },
    "89cc2257d1ea4e63a889c0c4c3b20b62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95713e453b7945d4bf3980f1ae512033": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9704b43fd02d47be994fefadbde6dfa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "981666d6021845389f9fa1a0ff5c7997": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1198949157d045b4b1d56330c21f861b",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32421abdc8fa41d8917203f6bfad586d",
      "value": 548105171
     }
    },
    "9b37ca3bdb944f2b95e65fdb4412e7dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb702efbe48642e1b9dd743faa0fcecf",
       "IPY_MODEL_c73ee2d1b39d4eeb90e976ee9710f859",
       "IPY_MODEL_13e8123052ad4599829fc081dac4eab3"
      ],
      "layout": "IPY_MODEL_272adc9fc3ba4c07943725eb800a18c1"
     }
    },
    "a865c5a49b0f4e84a6621b16d682edbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a90d112104994987b228ec0d4c7cd419": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2755e69b5484a28ade3d560840aecc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5defcb6637e4847a76bb982bd40dba4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6208163737e24a8ebc66fa95c7f4217f",
      "placeholder": "​",
      "style": "IPY_MODEL_30e53c8500a14fb880c2ab52007d1087",
      "value": " 124/124 [00:00&lt;00:00, 2.79kB/s]"
     }
    },
    "c21ba829e49b4824a83d9e79ec5b9892": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2ce3a86d5b34b2eb263fedcbbce72a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c55ebb7c205d40ba8c9f2682dd169f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ff7229e899e4afab9b5ee0b1597f5e5",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08b2a881d8b44bca9eabbe25558d2b10",
      "value": 1042301
     }
    },
    "c73ee2d1b39d4eeb90e976ee9710f859": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f073c1fbf7724c689cfe92ed50e43254",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d666d404db714d7f94a67d434b54a22b",
      "value": 456318
     }
    },
    "ce75e14ca9384d04bb297b6be28c36e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1d36249ca9b430dad8d8327138b8389": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d276c9530d9f456e9ae9c0b3a931cb93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce75e14ca9384d04bb297b6be28c36e5",
      "placeholder": "​",
      "style": "IPY_MODEL_267fca47895e493e82c9ef0568847b18",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 18.4MB/s]"
     }
    },
    "d3419184787645f9b617dc18ccaf315a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d54bba9da6d54cbd8e83eec29dae02c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c094717d594d37985f58c51527db59",
      "placeholder": "​",
      "style": "IPY_MODEL_9704b43fd02d47be994fefadbde6dfa2",
      "value": " 548M/548M [00:06&lt;00:00, 179MB/s]"
     }
    },
    "d5c7b0d2a6ff43eca128a12113300bd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5553fd97122a493fb99e3af859d16ddf",
       "IPY_MODEL_4dd5d713b82742df9b120193752b769c",
       "IPY_MODEL_340a9c92995a4189b69e459b033ae946"
      ],
      "layout": "IPY_MODEL_89cc2257d1ea4e63a889c0c4c3b20b62"
     }
    },
    "d666d404db714d7f94a67d434b54a22b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d767ae133f1a47bab5a40f4d8ac0d272": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e240513c73c2418f835465c3a93683f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5075a3f50b34526bdd4e215488341af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f7b0750af3646b787026494a1872a1f",
      "placeholder": "​",
      "style": "IPY_MODEL_6586f332f4104d8d914b8d390049310b",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "e75523e3b32a449bb5f08ab49d57a473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea6138d3d54f483587595bdda5211215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb702efbe48642e1b9dd743faa0fcecf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90d112104994987b228ec0d4c7cd419",
      "placeholder": "​",
      "style": "IPY_MODEL_ffb3be45dd1a4454bf59bb7ffb1e2413",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "f073c1fbf7724c689cfe92ed50e43254": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f32ea620bbc342dab3179e632f8fbe44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2755e69b5484a28ade3d560840aecc5",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7413bf18522241d3807a01f016d6139e",
      "value": 1355256
     }
    },
    "f36e03e930584b7d821b4184c54a5810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fba7c9b3bf9b409a9166cae7fc5fdba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c3f06e311c648569f17cf18f9c2f4f3",
       "IPY_MODEL_c55ebb7c205d40ba8c9f2682dd169f37",
       "IPY_MODEL_80bce29dfc73463c9b4cbeaea636ed74"
      ],
      "layout": "IPY_MODEL_4193f94efd9e457483fc4a2b03a3c07d"
     }
    },
    "ffb3be45dd1a4454bf59bb7ffb1e2413": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
