{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cdb984-1bd1-4c62-afb1-fac9c8677cbe",
   "metadata": {},
   "source": [
    "## Model Grafting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb18fd7-3c76-4cdd-91ef-a31410244c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    # %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "    renderer = \"colab\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from IPython import get_ipython\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    renderer = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7850beb2-bf91-4b0f-a597-f69be3275959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: poetry in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: build<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.10.0)\n",
      "Requirement already satisfied: cachecontrol[filecache]<0.13.0,>=0.12.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.12.14)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.21.5)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.12.2)\n",
      "Requirement already satisfied: html5lib<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.1)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.17.3)\n",
      "Requirement already satisfied: keyring<24.0.0,>=23.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.13.1)\n",
      "Requirement already satisfied: lockfile<0.13.0,>=0.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.12.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.1)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.8.0)\n",
      "Requirement already satisfied: poetry-core==1.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.6.1)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.4.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.0.post1)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.11.8)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2023.8.7)\n",
      "Collecting urllib3<2.0.0,>=1.26.0 (from poetry)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (20.24.3)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.13.0,>=0.12.9->poetry) (1.0.5)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cleo<3.0.0,>=2.0.0->poetry) (2.15.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (0.19.3)\n",
      "Requirement already satisfied: jaraco.classes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (5.2.0)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (2023.5.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from virtualenv<21.0.0,>=20.22.0->poetry) (0.3.7)\n",
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<24.0.0,>=23.9.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (39.0.2)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaraco.classes->keyring<24.0.0,>=23.9.0->poetry) (10.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (2.21)\n",
      "Installing collected packages: urllib3, platformdirs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.8.0\n",
      "    Uninstalling platformdirs-3.8.0:\n",
      "      Successfully uninstalled platformdirs-3.8.0\n",
      "Successfully installed platformdirs-3.10.0 urllib3-1.26.16\n",
      "Installing dependencies from lock file\n",
      "\n",
      "Package operations: 0 installs, 2 updates, 0 removals\n",
      "\n",
      "  • Updating urllib3 (1.26.16 -> 2.0.3)\n",
      "  • Updating platformdirs (3.10.0 -> 3.8.0)\n",
      "\n",
      "Installing the current project: transformer-lens (0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "docker-compose 1.29.2 requires jsonschema<4,>=2.5.1, but you have jsonschema 4.17.3 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "docker-compose 1.29.2 requires websocket-client<1,>=0.32.0, but you have websocket-client 1.6.1 which is incompatible.\n",
      "sagemaker 2.155.0 requires attrs<23,>=20.3.0, but you have attrs 23.1.0 which is incompatible.\n",
      "sagemaker 2.155.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.2.0 which is incompatible.\n",
      "sagemaker 2.155.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.23.3 which is incompatible.\n",
      "sagemaker 2.155.0 requires PyYAML==5.4.1, but you have pyyaml 6.0 which is incompatible.\n",
      "sparkmagic 0.20.5 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../\n",
    "pip install poetry\n",
    "poetry install\n",
    "cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d3d52e-646f-4715-b40a-bfff0987715e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = renderer\n",
    "\n",
    "\n",
    "# Import stuff\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "import ast\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import pickle\n",
    "if IN_COLAB: \n",
    "    import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens.utilities import devices\n",
    "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce11fc08-03af-4ce0-a2b8-8434f23bbead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6db405b-b60c-45c5-94e8-33eb312ac4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "def cuda():\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "def get_device(): \n",
    "    return \"cuda\" if cuda() else \"cpu\"\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Function to load a pickle object from a file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4c1d8-eaa7-4dd6-9511-f163b28eea66",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- fold_ln: Whether to fold in the LayerNorm weights to the subsequent linear layer. This does not change the computation.\n",
    "- center_writing_weights: Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNormthis doesn't change the computation.\n",
    "- center_unembed : Whether to center W_U (ie set mean to be zero). Softmax is translation invariant so this doesn't affect log probs or loss, but does change logits. Defaults to True.\n",
    "- refactor_factored_attn_matrices: Whether to convert the factoredmatrices (W_Q & W_K, and W_O & W_V) to be \"even\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026991d1-9be3-40cc-b8cf-cbadf7792d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-large\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        center_unembed=True,  \n",
    "        center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "        fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a3dc3-172c-4790-9aab-09db6333403a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use the CounterFact Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11446ada-fe11-453f-a45a-3ded4f64843d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://rome.baulab.info/data/dsets/counterfact.json\"\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "response = requests.get(url)\n",
    "dataset = response.json()\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, idx = None): \n",
    "    if idx is None: \n",
    "        prompt = \"The {} is located in\"\n",
    "        subject = \"Eiffel Tower\"\n",
    "        target = \"Paris\"\n",
    "    else: \n",
    "        sample = dataset[idx]\n",
    "        prompt = sample[\"requested_rewrite\"][\"prompt\"]\n",
    "        subject = sample[\"requested_rewrite\"][\"subject\"]\n",
    "        target = sample[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "    return prompt, subject, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c608fb-b179-4fe6-9365-7b90d6a6ded4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ablation Methods: \n",
    "- Noise ablation \n",
    "- Resample ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2601b6e-6a51-42bf-9285-ea45b487285f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def noise_ablation(prompt, subject, target, n_noise_samples=5, vx=3):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    \n",
    "    #shape: batch, n_tokens, embedding_dim\n",
    "    subject_embedding = model.embed(subject_tokens)\n",
    "    _, n_tokens, embedding_dim = subject_embedding.shape\n",
    "    \n",
    "    #noise: N(0,v), v = 3*std(embedding)\n",
    "    embedding = model.W_E\n",
    "    v = vx*torch.std(embedding, dim=0) #for each v in V\n",
    "    noise = torch.randn(\n",
    "        (n_noise_samples, n_tokens, embedding_dim)\n",
    "    ).to(device) + v\n",
    "    \n",
    "    subject_embedding_w_noise = subject_embedding + noise\n",
    "    \n",
    "    #shape: batch, n_tokens, vocab_size (logits)\n",
    "    unembedded_subject = model.unembed(subject_embedding_w_noise)\n",
    "\n",
    "    noisy_subject_tokens = torch.argmax(unembedded_subject, dim=-1)\n",
    "    noisy_subject_str = [\n",
    "        model.to_string(nst) for nst in noisy_subject_tokens\n",
    "    ]\n",
    "    true_prompt = prompt.format(subject)\n",
    "    corrupted_prompts = [\n",
    "        prompt.format(nss) for nss in noisy_subject_str\n",
    "    ]\n",
    "    return true_prompt, corrupted_prompts, target\n",
    "\n",
    "def resample_ablation(prompt, subject, target, n_noise_samples=20):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    embedding = model.W_E\n",
    "    #we select n random rows from the embedding matrix\n",
    "    permutations = torch.randperm(embedding.size(0))[:n_noise_samples]\n",
    "    random_samples = embedding[permutations]\n",
    "    #unsqueeze a token dimension between batch and embedding dims\n",
    "    random_samples = random_samples.unsqueeze(dim=1)\n",
    "    #we de-embed these rows\n",
    "    random_embeddings = model.unembed(random_samples)\n",
    "    random_tokens = torch.argmax(random_embeddings, dim=-1)\n",
    "    random_subject_str = [\n",
    "        model.to_string(t) for t in random_tokens\n",
    "    ]\n",
    "    corrupted_facts = [\n",
    "        prompt.format(s) for s in random_subject_str\n",
    "    ]\n",
    "    true_fact = prompt.format(subject)\n",
    "    return true_fact, corrupted_facts, target\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8f8accf-be5f-4546-9b69-717c8f87bc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_from_left(tokens : torch.tensor, maxlen:int):\n",
    "    pad_token = model.tokenizer.pad_token_id\n",
    "    padded_tokenized_inputs = torch.zeros(tokens.shape[0], maxlen)\n",
    "    \n",
    "    n_pads = maxlen - tokens.shape[-1]\n",
    "    padded_tokenized_inputs[:,n_pads] = pad_token\n",
    "    padded_tokenized_inputs[:,n_pads:] = tokens\n",
    "    return padded_tokenized_inputs.long()\n",
    "\n",
    "def pad_to_same_length(clean_tokens, corrupted_tokens): \n",
    "    \n",
    "    maxlen = max([clean_tokens.shape[-1], corrupted_tokens.shape[-1]])\n",
    "    \n",
    "    if clean_tokens.shape[-1] > corrupted_tokens.shape[-1]: \n",
    "        corrupted_tokens = pad_from_left(corrupted_tokens, maxlen)\n",
    "    elif clean_tokens.shape[-1] < corrupted_tokens.shape[-1]: \n",
    "        clean_tokens = pad_from_left(clean_tokens, maxlen)\n",
    "    return clean_tokens, corrupted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a49d7aa-2980-4f0b-adf0-b32627fd34f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "possible_patches = [\n",
    "    'ln1.hook_scale',\n",
    "    'ln1.hook_normalized',\n",
    "    'attn.hook_q',\n",
    "    'attn.hook_k',\n",
    "    'attn.hook_v',\n",
    "    # 'attn.hook_attn_scores',\n",
    "    # 'attn.hook_pattern',\n",
    "    # 'attn.hook_z',\n",
    "    # 'hook_attn_out',\n",
    "    # 'hook_resid_mid',\n",
    "    'ln2.hook_scale',\n",
    "    'ln2.hook_normalized',\n",
    "    'mlp.hook_pre',\n",
    "    'mlp.hook_post',\n",
    "    # 'hook_mlp_out',\n",
    "    'hook_resid_post'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "17619d02-1f75-4bbf-80eb-c2c15a6dbfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def patch(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    cache):\n",
    "    corrupted_residual_component[:, pos, :] = cache[hook.name][:, pos, :]\n",
    "    \"\"\"\n",
    "    Restore a patch in clean run. \n",
    "    \"\"\"\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def unembedding_function(residual_stack, cache, mlp=False) -> float:\n",
    "    #we are only interested in applying the layer norm of the final layer on the final token\n",
    "    #shape: [74, 5, 10, 1280] = n_layers, prompts, tokens, d_model\n",
    "    z = cache.apply_ln_to_stack(residual_stack, layer = -1, mlp_input=mlp)\n",
    "    z = z @ model.W_U\n",
    "    return z\n",
    "\n",
    "\n",
    "def initialize_graft(clean_cache, corrupted_cache,target_idx):\n",
    "    #should we initialize the graft using a decomposition of the residual logits\n",
    "    residual_clean_stack, clean_labels = clean_cache.decompose_resid(layer=-1, mode=\"attn_out\", return_labels=True)\n",
    "    residual_corrupted_stack, corrupted_labels = corrupted_cache.decompose_resid(layer=-1, mode=\"attn_out\", return_labels=True)\n",
    "    assert clean_labels==corrupted_labels\n",
    "    #shape: [74, 4, 9, 50257] = n_layers, batch, tokens, vocab_size\n",
    "    clean_logit_contributions_to_residual = unembedding_function(residual_clean_stack, clean_cache)[:,:,-1,:]\n",
    "    corrupted_logit_contributions_to_residual = unembedding_function(residual_corrupted_stack, corrupted_cache)[:,:,-1,:]\n",
    "        \n",
    "    target_idx_expanded = target_idx.repeat(clean_logit_contributions_to_residual.shape[0],1,1)\n",
    "    #center the logits\n",
    "    clean_logit_contributions_to_residual = (clean_logit_contributions_to_residual.gather(index=target_idx_expanded, dim=-1) \n",
    "                                             - clean_logit_contributions_to_residual.mean(dim=-1, keepdim=True))\n",
    "    \n",
    "    corrupted_logit_contributions_to_residual = (corrupted_logit_contributions_to_residual.gather(index=target_idx_expanded, dim=-1)\n",
    "                                                        - corrupted_logit_contributions_to_residual.mean(dim=-1, keepdim=True))\n",
    "    \n",
    "\n",
    "def similarity_loss(logits_original, logits_grafted, alpha=1.0, beta=1.0):\n",
    "    #TODO: should we be using top n logits only\n",
    "    probs_original = F.softmax(logits_original, dim=-1)\n",
    "    probs_grafted = F.softmax(logits_grafted, dim=-1)\n",
    "    kl_loss = F.kl_div(probs_grafted.log(), probs_original, reduction='batchmean')\n",
    "    \n",
    "    mse_loss = F.mse_loss(logits_grafted, logits_original)\n",
    "    total_loss = alpha * kl_loss + beta * mse_loss\n",
    "    \n",
    "    return total_loss\n",
    "        \n",
    "def run_w_graft(graft, model, all_activation_keys, clean_cache, corrupted_tokens): \n",
    "    fwd_hooks = []\n",
    "    for i in range(graft.shape[0]): \n",
    "        patch_name = all_activation_keys[i]\n",
    "        for tok_pos in range(graft.shape[1]): \n",
    "            graft_val = graft[i, tok_pos]\n",
    "            if graft_val==1: \n",
    "                hook_fn = partial(patch, pos=tok_pos, cache=clean_cache)\n",
    "                hook = (patch_name, hook_fn)\n",
    "                fwd_hooks.append(hook)\n",
    "\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        corrupted_tokens,\n",
    "        fwd_hooks=fwd_hooks,\n",
    "        return_type=\"logits\"\n",
    "    )   \n",
    "    patched_logits = patched_logits[:,-1,:]\n",
    "    return patched_logits\n",
    "\n",
    "    \n",
    "def learn_graft(\n",
    "             clean_prompt: str,\n",
    "             corrupted_prompts: List[str],\n",
    "             target: str,\n",
    "             learning_rate=10,\n",
    "             l1_penalty=0.001,\n",
    "             num_epochs=10):\n",
    "    \n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    #for now, use single corruption\n",
    "    corrupted_tokens = corrupted_tokens[0].unsqueeze(0)\n",
    "    \n",
    "    target_token_idx = model.to_tokens(target)[:,1] #remember to slice out the start pad token\n",
    "    #we pad the clean and corrupted tokens to the same length (pad from left)\n",
    "    clean_tokens, corrupted_tokens = pad_to_same_length(clean_tokens, corrupted_tokens)\n",
    "    \n",
    "    #repeat clean_tokens to match corrupted_tokens\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token_idx = target_token_idx.expand(corrupted_tokens.shape[0], -1)\n",
    "\n",
    "    _, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens, return_type=\"logits\")\n",
    "    clean_logits = clean_logits[:,-1,:]\n",
    "    # clean_target_logit = clean_logits[:,target_token_idx]\n",
    "    # clean_loss = F.cross_entropy(clean_logits.squeeze(), target_token_idx.squeeze())\n",
    "    # print(clean_cache.keys())\n",
    "    \n",
    "    token_positions = torch.arange(corrupted_tokens.shape[-1], device=device)\n",
    "    all_activation_keys = []\n",
    "    for layer in range(model.cfg.n_layers): \n",
    "        for activation_patch in possible_patches:\n",
    "            patch_name = f\"blocks.{layer}.{activation_patch}\"\n",
    "            all_activation_keys.append(patch_name)\n",
    "\n",
    "    \n",
    "    # graft_base = initialize_graft(clean_cache, corrupted_cache, target_token_idx)\n",
    "    graft_base_random = torch.rand(len(all_activation_keys), token_positions.shape[-1], device=device)\n",
    "    threshold = 0.1  # Adjust this threshold value as needed\n",
    "    # graft_base = (graft_base_random < threshold).long()\n",
    "    transformed_graft = (torch.sigmoid(graft_base_random) < threshold).long()\n",
    "    \n",
    "    # graft_base = torch.ones(size=(len(all_activation_keys),token_positions.shape[-1]), device=device)\n",
    "    # S = torch.normal(mean=0.1, std=0.001, size=(len(all_activation_keys), token_positions.shape[0]), requires_grad=True, device=device)\n",
    "    \n",
    "    optimizer = optim.SGD([transformed_graft], lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # graft = graft_base * (1 - torch.sigmoid(S)) + (1 - graft_base) * torch.sigmoid(S)\n",
    "        graft = (torch.sigmoid(transformed_graft) < 0.5).long()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            patched_logits = run_w_graft(\n",
    "                graft,\n",
    "                model,\n",
    "                all_activation_keys,\n",
    "                clean_cache,\n",
    "                corrupted_tokens\n",
    "            )\n",
    "        \n",
    "        similarity_loss_val = similarity_loss(clean_logits, patched_logits, alpha=1.0, beta=1.0)\n",
    "        regularized_loss = l1_penalty * torch.sum(torch.abs(graft))\n",
    "        total_loss = similarity_loss_val #+ regularized_loss\n",
    "\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Similarity Loss: {similarity_loss_val.item()} - L1 Loss: {regularized_loss.item()} - Sparisty: {graft.sum() / graft.flatten().shape[-1]}\")\n",
    "\n",
    "    return graft\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "39fe50ed-e7b9-4285-a016-cde0ff685641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [2/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [3/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [4/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [5/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [6/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [7/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [8/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [9/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n",
      "Epoch [10/10] - Similarity Loss: 36.952144622802734 - L1 Loss: 0.0 - Sparisty: 0.0\n"
     ]
    }
   ],
   "source": [
    "clean_prompt, subject, target = sample_dataset(dataset)\n",
    "original_fact, corrupted_facts, target = resample_ablation(clean_prompt, subject, target, n_noise_samples=10)\n",
    "\n",
    "graft = learn_graft(original_fact, corrupted_facts, target,\n",
    "                    learning_rate=0.001,\n",
    "                     l1_penalty=0.001,\n",
    "                     num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e538211-7562-4993-a102-a647bf10c81a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4997, device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35fc0e-6385-41aa-8bae-3c3f1f760058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
