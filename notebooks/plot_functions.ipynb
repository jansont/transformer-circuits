{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be8942-1f6e-4b6b-9c4e-b76e5c437ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_on_logit_attribitions(dataset, n, activation_to_ablate): \n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:n]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        result = compensatory_effect(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            corrupted_ablation=True, \n",
    "            activation_to_ablate = activation_to_ablate,\n",
    "            mode=\"all\",\n",
    "            mlp_input=False,\n",
    "            early_exit_1 = True\n",
    "        )\n",
    "        results.append(result)\n",
    "    return results\n",
    "results = run_experiment_on_logit_attribitions(dataset, n=100, activation_to_ablate=\"attn_out\") \n",
    "\n",
    "\n",
    "def plot_logit_attribution_by_layer(results, attribution_type=\"attn\", use_target=True):\n",
    "    # labels, target_residual_clean_logit, mle_residual_corrupted_logit, target_residual_clean_logit, mle_residual_clean_logit\n",
    "    layer_names = results[0][0]\n",
    "    idx_of_interest = torch.tensor([attribution_type in l in l for l in layer_names])\n",
    "    n_ablations = idx_of_interest.sum()\n",
    "    \n",
    "    all_residual_clean_logit = torch.zeros((len(results), n_ablations))\n",
    "    all_residual_corrupted_logit = torch.zeros((len(results), n_ablations))\n",
    "    delta_residual_logit = torch.zeros((len(results), n_ablations))\n",
    "    \n",
    "    for i,result in enumerate(results): \n",
    "        \n",
    "        (labels,target_residual_corrupted_logit,mle_residual_corrupted_logit,target_residual_clean_logit, mle_residual_clean_logit) = result\n",
    "        if use_target: \n",
    "            residual_clean_logit = target_residual_clean_logit.squeeze().mean(dim=-1)\n",
    "            residual_corrupted_logit = target_residual_corrupted_logit.squeeze().mean(dim=-1)\n",
    "        else: \n",
    "            residual_clean_logit = mle_residual_clean_logit.squeeze().mean(dim=-1)\n",
    "            residual_corrupted_logit = mle_residual_corrupted_logit.squeeze().mean(dim=-1)\n",
    "\n",
    "        all_residual_clean_logit[i] = residual_clean_logit[idx_of_interest]\n",
    "        all_residual_corrupted_logit[i] = residual_corrupted_logit[idx_of_interest]\n",
    "        delta_residual_logit[i] = all_residual_clean_logit[i] - all_residual_corrupted_logit[i]\n",
    "    \n",
    "            \n",
    "            \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "    for row in all_residual_clean_logit:\n",
    "        axes[0].plot(row, color='grey', linewidth=0.5)\n",
    "    for row in all_residual_corrupted_logit:\n",
    "        axes[1].plot(row, color='grey', linewidth=0.5)\n",
    "    for row in delta_residual_logit:\n",
    "        axes[2].plot(row, color='grey', linewidth=0.5)   \n",
    "\n",
    "    axes[0].plot(torch.mean(all_residual_clean_logit, dim=0), color='blue', linewidth=2, label='Mean')\n",
    "    axes[1].plot(torch.mean(all_residual_corrupted_logit, dim=0), color='blue', linewidth=2, label='Mean')\n",
    "    axes[2].plot(torch.mean(delta_residual_logit, dim=0), color='blue', linewidth=2, label='Mean')\n",
    "\n",
    "    # Set titles and labels\n",
    "    axes[0].set_title('Clean prompt')\n",
    "    axes[0].set_xlabel('Layer Index (Attention)')\n",
    "    axes[0].set_ylabel('Logit Contribution')\n",
    "    axes[1].set_title('Corrupted prompt (resample ablation)')\n",
    "    axes[1].set_xlabel('Layer Index (Attention)')\n",
    "    axes[1].set_ylabel('Logit Contribution')\n",
    "    axes[2].set_title('Delta')\n",
    "    axes[2].set_xlabel('Layer Index (Attention)')\n",
    "    axes[2].set_ylabel('Logit Contribution')\n",
    "\n",
    "    # Add legends\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    axes[2].legend()\n",
    "\n",
    "    # Set y-limits manually\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(-0.5, 1.5)  # Replace ymin and ymax with your desired y-limits\n",
    "\n",
    "    # Add a master title\n",
    "    fig.suptitle('Mean Decomposition of Contributions to Logits of Target Token (Attention)', fontsize=16)\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_logit_attribution_by_layer(results, attribution_type=\"attn\", use_target=True)\n",
    "        \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p38",
   "language": "python",
   "name": "conda_mxnet_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
