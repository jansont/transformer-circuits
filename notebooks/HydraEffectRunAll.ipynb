{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D06wUjxTZ8ej",
    "outputId": "ee034021-4479-432b-8a21-de6d34adb14a",
    "tags": []
   },
   "source": [
    "# Causal Analysis of Symmetric Facts\n",
    "We analyze where facts might be locateed in an LLM. A fact is represented by a tuple of subject, relation, object (s,r,o). We also investigate where an inverse fact mis located. An inverse fact is represented by the tuple (o, r^-1, s). For example, the fact \"Paris is the capital of France\" has the inverse fact \"France's capital is France\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    # %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "    renderer = \"colab\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from IPython import get_ipython\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    renderer = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: poetry in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: build<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.10.0)\n",
      "Requirement already satisfied: cachecontrol[filecache]<0.13.0,>=0.12.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.12.14)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.21.5)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.12.2)\n",
      "Requirement already satisfied: html5lib<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.1)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.17.3)\n",
      "Requirement already satisfied: keyring<24.0.0,>=23.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.13.1)\n",
      "Requirement already satisfied: lockfile<0.13.0,>=0.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.12.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.1)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.8.0)\n",
      "Requirement already satisfied: poetry-core==1.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.6.1)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.4.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.11.8)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2023.8.7)\n",
      "Collecting urllib3<2.0.0,>=1.26.0 (from poetry)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (20.24.3)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.13.0,>=0.12.9->poetry) (1.0.5)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cleo<3.0.0,>=2.0.0->poetry) (2.15.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (0.19.3)\n",
      "Requirement already satisfied: jaraco.classes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (5.2.0)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (2023.5.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from virtualenv<21.0.0,>=20.22.0->poetry) (0.3.7)\n",
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<24.0.0,>=23.9.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (39.0.2)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaraco.classes->keyring<24.0.0,>=23.9.0->poetry) (10.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (2.21)\n",
      "Installing collected packages: urllib3, platformdirs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.8.0\n",
      "    Uninstalling platformdirs-3.8.0:\n",
      "      Successfully uninstalled platformdirs-3.8.0\n",
      "Successfully installed platformdirs-3.10.0 urllib3-1.26.16\n",
      "Installing dependencies from lock file\n",
      "\n",
      "Package operations: 0 installs, 2 updates, 0 removals\n",
      "\n",
      "  • Updating urllib3 (1.26.16 -> 2.0.3)\n",
      "  • Updating platformdirs (3.10.0 -> 3.8.0)\n",
      "\n",
      "Installing the current project: transformer-lens (0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.167.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.2.0 which is incompatible.\n",
      "sagemaker 2.167.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.23.3 which is incompatible.\n",
      "sparkmagic 0.20.5 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: poetry in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: build<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.10.0)\n",
      "Requirement already satisfied: cachecontrol[filecache]<0.13.0,>=0.12.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.12.14)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.21.5)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.12.2)\n",
      "Requirement already satisfied: html5lib<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.1)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.17.3)\n",
      "Requirement already satisfied: keyring<24.0.0,>=23.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.13.1)\n",
      "Requirement already satisfied: lockfile<0.13.0,>=0.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.12.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.1)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.8.0)\n",
      "Requirement already satisfied: poetry-core==1.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.6.1)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.4.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.11.8)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2023.8.7)\n",
      "Collecting urllib3<2.0.0,>=1.26.0 (from poetry)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (20.24.3)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.13.0,>=0.12.9->poetry) (1.0.5)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cleo<3.0.0,>=2.0.0->poetry) (2.15.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib<2.0,>=1.0->poetry) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.10.0->poetry) (0.19.3)\n",
      "Requirement already satisfied: jaraco.classes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (5.2.0)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<24.0.0,>=23.9.0->poetry) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (2023.5.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from virtualenv<21.0.0,>=20.22.0->poetry) (0.3.7)\n",
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<24.0.0,>=23.9.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (39.0.2)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaraco.classes->keyring<24.0.0,>=23.9.0->poetry) (10.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<24.0.0,>=23.9.0->poetry) (2.21)\n",
      "Installing collected packages: urllib3, platformdirs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.8.0\n",
      "    Uninstalling platformdirs-3.8.0:\n",
      "      Successfully uninstalled platformdirs-3.8.0\n",
      "Successfully installed platformdirs-3.10.0 urllib3-1.26.16\n",
      "Installing dependencies from lock file\n",
      "\n",
      "Package operations: 0 installs, 2 updates, 0 removals\n",
      "\n",
      "  • Updating urllib3 (1.26.16 -> 2.0.3)\n",
      "  • Updating platformdirs (3.10.0 -> 3.8.0)\n",
      "\n",
      "Installing the current project: transformer-lens (0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.167.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.2.0 which is incompatible.\n",
      "sagemaker 2.167.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.23.3 which is incompatible.\n",
      "sparkmagic 0.20.5 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../\n",
    "pip install poetry\n",
    "poetry install\n",
    "cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SXYnmmdiKv4V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "epoi7Cb7Kv4V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import stuff\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "import subprocess\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "c5KcrsoJKv4W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IN_COLAB: \n",
    "    import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens.utilities import devices\n",
    "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdw4b_oKKv4W"
   },
   "source": [
    "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbmPPShhKv4W",
    "outputId": "f71a265a-00c8-427e-dc22-a5f8a5f6e056",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fbb22d548e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fbb22d548e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F83ndvw3Kv4W"
   },
   "source": [
    "Plotting helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9QMgUkYMKv4W",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "def cuda():\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "def get_device(): \n",
    "    return \"cuda\" if cuda() else \"cpu\"\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Function to load a pickle object from a file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "    return loaded_data\n",
    "\n",
    "def get_gpu_usage(): \n",
    "    subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "- fold_ln: Whether to fold in the LayerNorm weights to the subsequent linear layer. This does not change the computation.\n",
    "- center_writing_weights: Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNormthis doesn't change the computation.\n",
    "- center_unembed : Whether to center W_U (ie set mean to be zero). Softmax is translation invariant so this doesn't affect log probs or loss, but does change logits. Defaults to True.\n",
    "- refactor_factored_attn_matrices: Whether to convert the factoredmatrices (W_Q & W_K, and W_O & W_V) to be \"even\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "d5c7b0d2a6ff43eca128a12113300bd9",
      "5553fd97122a493fb99e3af859d16ddf",
      "4dd5d713b82742df9b120193752b769c",
      "340a9c92995a4189b69e459b033ae946",
      "89cc2257d1ea4e63a889c0c4c3b20b62",
      "4a08436dc4c54a1ab2219e66058f68b5",
      "e75523e3b32a449bb5f08ab49d57a473",
      "50fd7bca1a95484fa13c37242a988fc3",
      "6309b4c1e92f46e9b09f9615d20705d8",
      "d3419184787645f9b617dc18ccaf315a",
      "45df25e66c6041d6b0470865f43d3030",
      "7e9d95a57d194b379f33467895fc8883",
      "7eecaf2e1f6d401b9d85abf1438cde9b",
      "981666d6021845389f9fa1a0ff5c7997",
      "d54bba9da6d54cbd8e83eec29dae02c4",
      "2df76b7b518948d88d9aba8f670e7f9c",
      "c2ce3a86d5b34b2eb263fedcbbce72a0",
      "4baa43529767484fb04fb1f9e94c1aab",
      "1198949157d045b4b1d56330c21f861b",
      "32421abdc8fa41d8917203f6bfad586d",
      "05c094717d594d37985f58c51527db59",
      "9704b43fd02d47be994fefadbde6dfa2",
      "57a96c26b3944e7486bb2ae8c26d8973",
      "e5075a3f50b34526bdd4e215488341af",
      "0632336b81a043d09cab7a8b3774968c",
      "b5defcb6637e4847a76bb982bd40dba4",
      "c21ba829e49b4824a83d9e79ec5b9892",
      "5f7b0750af3646b787026494a1872a1f",
      "6586f332f4104d8d914b8d390049310b",
      "0f3cef4147924e129b11161067776df0",
      "03055d1e38844792bbd9750b2daf786a",
      "6208163737e24a8ebc66fa95c7f4217f",
      "30e53c8500a14fb880c2ab52007d1087",
      "fba7c9b3bf9b409a9166cae7fc5fdba1",
      "1c3f06e311c648569f17cf18f9c2f4f3",
      "c55ebb7c205d40ba8c9f2682dd169f37",
      "80bce29dfc73463c9b4cbeaea636ed74",
      "4193f94efd9e457483fc4a2b03a3c07d",
      "95713e453b7945d4bf3980f1ae512033",
      "d767ae133f1a47bab5a40f4d8ac0d272",
      "1ff7229e899e4afab9b5ee0b1597f5e5",
      "08b2a881d8b44bca9eabbe25558d2b10",
      "e240513c73c2418f835465c3a93683f2",
      "576faf57e7234808866d8ae0c273ecbd",
      "9b37ca3bdb944f2b95e65fdb4412e7dd",
      "eb702efbe48642e1b9dd743faa0fcecf",
      "c73ee2d1b39d4eeb90e976ee9710f859",
      "13e8123052ad4599829fc081dac4eab3",
      "272adc9fc3ba4c07943725eb800a18c1",
      "a90d112104994987b228ec0d4c7cd419",
      "ffb3be45dd1a4454bf59bb7ffb1e2413",
      "f073c1fbf7724c689cfe92ed50e43254",
      "d666d404db714d7f94a67d434b54a22b",
      "53ab2b48537d45f0a311d51bdf2f03ff",
      "f36e03e930584b7d821b4184c54a5810",
      "0336fd949a66477c9daaca7b78b5a3a3",
      "669e15f191394c0ca74630c025b164bc",
      "f32ea620bbc342dab3179e632f8fbe44",
      "d276c9530d9f456e9ae9c0b3a931cb93",
      "ea6138d3d54f483587595bdda5211215",
      "d1d36249ca9b430dad8d8327138b8389",
      "a865c5a49b0f4e84a6621b16d682edbd",
      "b2755e69b5484a28ade3d560840aecc5",
      "7413bf18522241d3807a01f016d6139e",
      "ce75e14ca9384d04bb297b6be28c36e5",
      "267fca47895e493e82c9ef0568847b18"
     ]
    },
    "id": "oyrZcBKMecVw",
    "outputId": "35c1fbee-6dad-4eb8-a7b1-e67cbb5497ac",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b099f7ec55247afb80f553af9364c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b099f7ec55247afb80f553af9364c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795be4df99dc4b54b8040cfccb59a45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795be4df99dc4b54b8040cfccb59a45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ded645c9c264e4ea8beb25dfcf9165b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ded645c9c264e4ea8beb25dfcf9165b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b8b8d3f62b4559bf3554b688b8c983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b8b8d3f62b4559bf3554b688b8c983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae9f40079724fd4b9a25777caae9fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae9f40079724fd4b9a25777caae9fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c007e88df5fe4802ae275766b5891764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c007e88df5fe4802ae275766b5891764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n",
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-large\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        center_unembed=True,  \n",
    "        center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "        fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use the CounterFact Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_json(\"data/fact_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, idx = None): \n",
    "    if idx is None: \n",
    "        prompt = \"The {} is located in\"\n",
    "        subject = \"Eiffel Tower\"\n",
    "        target = \"Paris\"\n",
    "    else: \n",
    "        sample = dataset[idx]\n",
    "        prompt = sample[\"requested_rewrite\"][\"prompt\"]\n",
    "        subject = sample[\"requested_rewrite\"][\"subject\"]\n",
    "        target = sample[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "    return prompt, subject, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = sample_dataset(dataset)\n",
    "prompt, subject, target = sample\n",
    "prompt = prompt.format(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(prompt).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Methods: \n",
    "- Noise ablation \n",
    "- Resample ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def noise_ablation(prompt, subject, target, n_noise_samples=5, vx=3):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    \n",
    "    #shape: batch, n_tokens, embedding_dim\n",
    "    subject_embedding = model.embed(subject_tokens)\n",
    "    _, n_tokens, embedding_dim = subject_embedding.shape\n",
    "    \n",
    "    #noise: N(0,v), v = 3*std(embedding)\n",
    "    embedding = model.W_E\n",
    "    v = vx*torch.std(embedding, dim=0) #for each v in V\n",
    "    noise = torch.randn(\n",
    "        (n_noise_samples, n_tokens, embedding_dim)\n",
    "    ).to(device) + v\n",
    "    \n",
    "    subject_embedding_w_noise = subject_embedding + noise\n",
    "    \n",
    "    #shape: batch, n_tokens, vocab_size (logits)\n",
    "    unembedded_subject = model.unembed(subject_embedding_w_noise)\n",
    "\n",
    "    noisy_subject_tokens = torch.argmax(unembedded_subject, dim=-1)\n",
    "    noisy_subject_str = [\n",
    "        model.to_string(nst) for nst in noisy_subject_tokens\n",
    "    ]\n",
    "    true_prompt = prompt.format(subject)\n",
    "    corrupted_prompts = [\n",
    "        prompt.format(nss) for nss in noisy_subject_str\n",
    "    ]\n",
    "    return true_prompt, corrupted_prompts, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in',\n",
       " ['The  Zan agre¶Unionfacts is located in',\n",
       "  'The benefjadOntfenande is located in',\n",
       "  'The Sharp smackitsch MAGeper is located in',\n",
       "  'The  Canceiasmrothdescriptionzek is located in',\n",
       "  'The idth nominal subsequ unemploy Conserv is located in'],\n",
       " 'Paris')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in',\n",
       " ['The  Zan agre¶Unionfacts is located in',\n",
       "  'The benefjadOntfenande is located in',\n",
       "  'The Sharp smackitsch MAGeper is located in',\n",
       "  'The  Canceiasmrothdescriptionzek is located in',\n",
       "  'The idth nominal subsequ unemploy Conserv is located in'],\n",
       " 'Paris')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt, subject, target = sample_dataset(dataset)\n",
    "noise_ablation(prompt, subject, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resample_ablation(prompt, subject, target, n_noise_samples=20):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    embedding = model.W_E\n",
    "    #we select n random rows from the embedding matrix\n",
    "    permutations = torch.randperm(embedding.size(0))[:n_noise_samples]\n",
    "    random_samples = embedding[permutations]\n",
    "    #unsqueeze a token dimension between batch and embedding dims\n",
    "    random_samples = random_samples.unsqueeze(dim=1)\n",
    "    #we de-embed these rows\n",
    "    random_embeddings = model.unembed(random_samples)\n",
    "    random_tokens = torch.argmax(random_embeddings, dim=-1)\n",
    "    random_subject_str = [\n",
    "        model.to_string(t) for t in random_tokens\n",
    "    ]\n",
    "    corrupted_facts = [\n",
    "        prompt.format(s) for s in random_subject_str\n",
    "    ]\n",
    "    true_fact = prompt.format(subject)\n",
    "    return true_fact, corrupted_facts, target\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in',\n",
       " ['The  CE is located in',\n",
       "  'The Graphics is located in',\n",
       "  'The  advancing is located in',\n",
       "  'The  curls is located in',\n",
       "  'The  cozy is located in'],\n",
       " 'Paris')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in',\n",
       " ['The  CE is located in',\n",
       "  'The Graphics is located in',\n",
       "  'The  advancing is located in',\n",
       "  'The  curls is located in',\n",
       "  'The  cozy is located in'],\n",
       " 'Paris')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_ablation(prompt, subject, target, n_noise_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_from_left(tokens : torch.tensor, maxlen:int):\n",
    "    pad_token = model.tokenizer.pad_token_id\n",
    "    padded_tokenized_inputs = torch.zeros(tokens.shape[0], maxlen)\n",
    "    \n",
    "    n_pads = maxlen - tokens.shape[-1]\n",
    "    padded_tokenized_inputs[:,n_pads] = pad_token\n",
    "    padded_tokenized_inputs[:,n_pads:] = tokens\n",
    "    return padded_tokenized_inputs.long()\n",
    "\n",
    "def pad_to_same_length(clean_tokens, corrupted_tokens): \n",
    "    \n",
    "    maxlen = max([clean_tokens.shape[-1], corrupted_tokens.shape[-1]])\n",
    "    \n",
    "    if clean_tokens.shape[-1] > corrupted_tokens.shape[-1]: \n",
    "        corrupted_tokens = pad_from_left(corrupted_tokens, maxlen)\n",
    "    elif clean_tokens.shape[-1] < corrupted_tokens.shape[-1]: \n",
    "        clean_tokens = pad_from_left(clean_tokens, maxlen)\n",
    "    return clean_tokens, corrupted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save(tensor, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(tensor, f)\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        loaded_tensor = pickle.load(f)\n",
    "    return loaded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unembedding_function(residual_stack, cache, mlp=False) -> float:\n",
    "    #we are only interested in applying the layer norm of the final layer on the final token\n",
    "    #shape: [74, 5, 10, 1280] = n_layers, prompts, tokens, d_model\n",
    "    z = cache.apply_ln_to_stack(residual_stack, layer = -1, mlp_input=mlp)\n",
    "    z = z @ model.W_U\n",
    "    return z\n",
    "\n",
    "\n",
    "def delta_ablate(cache,\n",
    "                  mle_token_idx,\n",
    "                  target_token_idx, \n",
    "                  mlp_input, \n",
    "                return_labels=False):\n",
    "    if return_labels: \n",
    "        residual_ablated_stack, labels = cache.decompose_resid(layer=-1, mlp_input=mlp_input, mode=\"all\", return_labels=True)\n",
    "    else: \n",
    "        residual_ablated_stack = cache.decompose_resid(layer=-1, mlp_input=mlp_input, mode=\"all\", return_labels=False)\n",
    "    \n",
    "    #shape: [74, 4, 9, 50257] = n_layers, batch, tokens, vocab_size\n",
    "    residual_logits = unembedding_function(residual_ablated_stack, cache)\n",
    "    residual_logits = residual_logits[:,:,-1,:]\n",
    "\n",
    "    target_idx_expanded = target_token_idx.repeat(residual_logits.shape[0],1,1)\n",
    "    mle_idx_expanded = mle_token_idx.repeat(residual_logits.shape[0],1,1)\n",
    "\n",
    "    target_residual_logits = residual_logits.gather(index=target_idx_expanded, dim=-1) - residual_logits.mean(dim=-1, keepdim=True)\n",
    "    mle_residual_logits = residual_logits.gather(index=mle_idx_expanded, dim=-1) - residual_logits.mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    if return_labels: \n",
    "        return labels, mle_residual_logits, target_residual_logits\n",
    "    return mle_residual_logits, target_residual_logits\n",
    "\n",
    "def patch_layer(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    cache):\n",
    "    corrupted_residual_component[:, :, :] = cache[hook.name][:, :, :]\n",
    "    \"\"\"\n",
    "    Restore a patch in clean run. \n",
    "    \"\"\"\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def run_all(clean_prompt: str,\n",
    "                         corrupted_prompts: List[str],\n",
    "                         target: str, \n",
    "                         corrupted_ablation=True, \n",
    "                         activation_to_ablate = \"attn_out\", \n",
    "                         mlp_input=False):\n",
    "    \"\"\"\n",
    "    activation_to_ablate: activation (resid_pre, attn_out, or mlp_out to ablate). resid_pre corresponds to the value of the residual\n",
    "    stream at a layer and a position. attn_out and mlp_out correspond to the quantities added into the residual stream at a given \n",
    "    attention or mlp head. The values in the residual stream are preserved but the quantities being injected are corrupted. \n",
    "    \n",
    "    mlp_input: Whether to include attn_out for the current\n",
    "    layer - essentially decomposing the residual stream that's input to the MLP input rather than the Attn input.\n",
    "    \n",
    "    mode (str): Values are \"all\", \"mlp\" or \"attn\". \"all\" returns all\n",
    "    components, \"mlp\" returns only the MLP components, and \"attn\" returns only the attention components.\n",
    "    \n",
    "    Here we get the compensatory effect of downstream layers following an ablation on the MLE token logits and the target token logits.\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    #we take first token for the target. Not ideal though\n",
    "    target_token_idx = model.to_tokens(target)[:,1] #remember to slice out the start pad token\n",
    "\n",
    "    #we pad the clean and corrupted tokens to the same length (pad from left)\n",
    "    clean_tokens, corrupted_tokens = pad_to_same_length(clean_tokens, corrupted_tokens)\n",
    "    #repeat clean_tokens to match corrupted_tokens which have multiple noise samples. \n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token_idx = target_token_idx.expand(corrupted_tokens.shape[0], -1)\n",
    "    \n",
    "    #run the model on the clean and corrupted tokens, saving the model states for each\n",
    "    #logits_shape: batch, n_tokens, vocab\n",
    "    \n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens, return_type=\"logits\")\n",
    "    corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
    "    #The difference is states is attributed to the corrupted input. The difference in logit attributions\n",
    "    #at each layer is the direct effect on that layer. \n",
    "    \n",
    "    #if we want to inverse the corruption: set corrupted_ablation to False. This runs the corrupted prompts\n",
    "    #and tracks the effect of restore clean states. For sake of clean code, we simply swap variables. \n",
    "    if not corrupted_ablation: \n",
    "        temp_cache = corrupted_cache\n",
    "        temp_logits = corrupted_logits\n",
    "        corrupted_cache = clean_cache\n",
    "        corrupted_logits = clean_logits\n",
    "        clean_cache = temp_cache\n",
    "        clean_logits = temp_logits\n",
    "    \n",
    "    mle_token_idx = torch.argmax(clean_logits[:,-1,:], dim=-1).unsqueeze(-1)\n",
    "    # layers, batch, 1 = direct effect at each layer due to the ablation\n",
    "    \n",
    "    clean_logits = clean_logits[:,-1,:]\n",
    "    corrupted_logits = corrupted_logits[:,-1,:]\n",
    "    \n",
    "#     print(clean_logits.shape)\n",
    "#     print(mle_token_idx.shape)\n",
    "#     print(clean_logits[mle_token_idx].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    clean_mle_logit = clean_logits[:,-1,mle_token_idx] - clean_logits[:,-1,:].mean(dim=-1, keepdim=True)\n",
    "    clean_target_logit = clean_logits[:,-1,target_token_idx]- clean_logits[:,-1,:].mean(dim=-1, keepdim=True)\n",
    "    corrupted_mle_logit = corrupted_logits[:,-1,mle_token_idx]- corrupted_logits[:,-1,:].mean(dim=-1, keepdim=True)\n",
    "    corrupted_target_logit = corrupted_logits[:,-1,target_token_idx]- corrupted_logits[:,-1,:].mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    clean_mle_logit = clean_mle_logit.mean(dim=0, keepdim=True).to(\"cpu\")\n",
    "    clean_target_logit = clean_target_logit.mean(dim=0, keepdim=True).to(\"cpu\")\n",
    "    corrupted_mle_logit = corrupted_mle_logit.mean(dim=0, keepdim=True).to(\"cpu\")\n",
    "    corrupted_target_logit = corrupted_target_logit.mean(dim=0, keepdim=True).to(\"cpu\")\n",
    "    \n",
    "    layer_names, clean_mle_residual_logits, clean_target_residual_logits = delta_ablate(clean_cache,\n",
    "                                                                          mle_token_idx,\n",
    "                                                                          target_token_idx, \n",
    "                                                                          mlp_input, \n",
    "                                                                         return_labels=True)\n",
    "    \n",
    "    corrupted_mle_residual_logits, corrupted_target_residual_logits = delta_ablate(corrupted_cache,\n",
    "                                                                              mle_token_idx,\n",
    "                                                                              target_token_idx, \n",
    "                                                                              mlp_input)\n",
    "    #average over batch\n",
    "    clean_mle_residual_logits = clean_mle_residual_logits.squeeze().mean(dim=-1)\n",
    "    clean_target_residual_logits = clean_target_residual_logits.squeeze().mean(dim=-1)\n",
    "    \n",
    "    corrupted_mle_residual_logits = corrupted_mle_residual_logits.squeeze().mean(dim=-1)\n",
    "    corrupted_target_residual_logits = corrupted_target_residual_logits.squeeze().mean(dim=-1)\n",
    "        \n",
    "    \n",
    "    total_ablated_mle_residual_logits = torch.zeros(model.cfg.n_layers, len(layer_names), \n",
    "                                          device=\"cpu\", dtype=torch.float32)\n",
    "    total_ablated_target_residual_logits = torch.zeros(model.cfg.n_layers,len(layer_names),\n",
    "                                             device=\"cpu\", dtype=torch.float32)\n",
    "    total_ablated_mle_logits = torch.zeros(model.cfg.n_layers,device=\"cpu\", dtype=torch.float32)\n",
    "    \n",
    "    total_ablated_target_logits = torch.zeros(model.cfg.n_layers,device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        #here we run the prompt, add a hook point from the corrupted_cache to ablate \n",
    "        #a position and layer. We then run with cache to get a saved state of the activations. \n",
    "        hook_fn = partial(patch_layer, cache=corrupted_cache)            \n",
    "        with model.hooks(\n",
    "            fwd_hooks = [(utils.get_act_name(activation_to_ablate, layer),hook_fn)]\n",
    "        ) as hooked_model:\n",
    "            restored_logits, ablated_cache = hooked_model.run_with_cache(clean_tokens, return_type=\"logits\")\n",
    "            # layers, batch, 1 = direct effect at each layer due to the ablation\n",
    "            ablated_mle_residual_logits, ablated_target_residual_logits  = delta_ablate(ablated_cache,\n",
    "                                                                                          mle_token_idx,\n",
    "                                                                                          target_token_idx, \n",
    "                                                                                          mlp_input\n",
    "            )\n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "        \n",
    "        \n",
    "        mle_logit = restored_logit[:, -1, mle_token_idx] - restored_logit[:,-1,:].mean(dim=-1, keepdim=True)\n",
    "        target_logit = restored_logit[:,-1,target_token_idx] - restored_logit[:,-1,:].mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        total_ablated_mle_logits[layer] = mle_logit.mean(dim=0, keepdim=True).to(\"cpu\")\n",
    "        total_ablated_target_logits = target_logit.mean(dim=0, keepdim=True).to(\"cpu\")\n",
    "        \n",
    "        total_ablated_mle_residual_logits[layer] = ablated_mle_residual_logits.squeeze().mean(dim=-1).to(\"cpu\")\n",
    "        total_ablated_target_residual_logits[layer] = ablated_target_residual_logits.squeeze().mean(dim=-1).to(\"cpu\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"layer_names\" : layer_names,\n",
    "        \"clean_mle_residual_logits\" : clean_mle_residual_logits.to(\"cpu\"),\n",
    "        \"clean_target_residual_logits\" : clean_target_residual_logits.to(\"cpu\"),\n",
    "        \"corrupted_mle_residual_logits\" : corrupted_mle_residual_logits.to(\"cpu\"),\n",
    "        \"corrupted_target_residual_logits\" : corrupted_target_residual_logits.to(\"cpu\"),\n",
    "        \"ablated_mle_residual_logits\" : total_ablated_mle_residual_logits.to(\"cpu\"),\n",
    "        \"ablated_target_residual_logits\" : total_ablated_target_residual_logits.to(\"cpu\"),\n",
    "        \"total_ablated_mle_logits\" : total_ablated_mle_logits.to(\"cpu\"),\n",
    "        \"total_ablated_target_logits\" : total_ablated_target_logits.to(\"cpu\"),\n",
    "        \n",
    "         \"clean_mle_logit\":clean_mle_logit,\n",
    "         \"clean_target_logit\":clean_target_logit,\n",
    "         \"corrupted_mle_logit\":corrupted_mle_logit,\n",
    "         \"corrupted_target_logit\":corrupted_target_logit,\n",
    "    \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(indices, activation_to_ablate): \n",
    "    # indices = list(range(len(dataset)))\n",
    "    # random.shuffle(indices)\n",
    "    # indices = indices[:n]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        print(true_fact)\n",
    "        result = run_all(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "            activation_to_ablate = activation_to_ablate,\n",
    "        )\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# results_attn_CE = run_experiment_on_delta_ablate(dataset, n=2, activation_to_ablate=\"attn_out\") \n",
    "# results_mlp = run_experiment_on_delta_ablate(dataset, n=10, activation_to_ablate=\"mlp_out\") \n",
    "# results_resid = run_experiment_on_delta_ablate(dataset, n=10, activation_to_ablate=\"attn_out\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12604/3712088978.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"results/{indices.min()}-{indices.max()}_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_to_ablate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attn_out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12604/1668587743.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(indices, activation_to_ablate)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_ablation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_noise_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         result = run_all(\n",
      "\u001b[0;32m/tmp/ipykernel_12604/1460949081.py\u001b[0m in \u001b[0;36mresample_ablation\u001b[0;34m(prompt, subject, target, n_noise_samples)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrandom_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#we de-embed these rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrandom_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munembed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mrandom_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     random_subject_str = [\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     56\u001b[0m     ) -> Float[torch.Tensor, \"batch pos d_vocab_out\"]:\n\u001b[1;32m     57\u001b[0m         return (\n\u001b[0;32m---> 58\u001b[0;31m             einsum(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;34m\"batch pos d_model, d_model vocab -> batch pos vocab\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mnew_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_equation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_equation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12604/3712088978.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"results/{indices.min()}-{indices.max()}_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_to_ablate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attn_out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12604/1668587743.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(indices, activation_to_ablate)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_ablation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_noise_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         result = run_all(\n",
      "\u001b[0;32m/tmp/ipykernel_12604/1460949081.py\u001b[0m in \u001b[0;36mresample_ablation\u001b[0;34m(prompt, subject, target, n_noise_samples)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrandom_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#we de-embed these rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrandom_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munembed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mrandom_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     random_subject_str = [\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     56\u001b[0m     ) -> Float[torch.Tensor, \"batch pos d_vocab_out\"]:\n\u001b[1;32m     57\u001b[0m         return (\n\u001b[0;32m---> 58\u001b[0;31m             einsum(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;34m\"batch pos d_model, d_model vocab -> batch pos vocab\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mnew_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_equation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_equation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
     ]
    }
   ],
   "source": [
    "index_batches = [np.arange(i, 100+i) for i in [0, 100, 200, 300, 400]]\n",
    "\n",
    "\n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_results.pickle\"\n",
    "    results = run_experiment(indices, activation_to_ablate=\"attn_out\")\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n",
    "# results_attn_CE = run_experiment_on_delta_ablate(dataset, n=2, activation_to_ablate=\"attn_out\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle_files = [f for f in os.listdir(\"results\") if f.split(\".\")[-1]==\"pickle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for file in pickle_files: \n",
    "    result = load(\"results/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_result(result): \n",
    "    layer_names = result[0][\"layer_names\"]\n",
    "    clean_mle_residual_logits = []\n",
    "    clean_target_residual_logits = []\n",
    "    corrupted_mle_residual_logits = []\n",
    "    corrupted_target_residual_logits = []\n",
    "    ablated_mle_residual_logits = []\n",
    "    ablated_target_residual_logits = []\n",
    "\n",
    "    for sample_result in result: \n",
    "        clean_mle_residual_logits.append(sample_result[\"clean_mle_residual_logits\"])\n",
    "        clean_target_residual_logits.append(sample_result[\"clean_target_residual_logits\"])\n",
    "        corrupted_mle_residual_logits.append(sample_result[\"corrupted_mle_residual_logits\"])\n",
    "        corrupted_target_residual_logits.append(sample_result[\"corrupted_target_residual_logits\"])\n",
    "        ablated_mle_residual_logits.append(sample_result[\"ablated_mle_residual_logits\"])\n",
    "        ablated_target_residual_logits.append(sample_result[\"ablated_target_residual_logits\"])\n",
    "        \n",
    "    clean_mle_residual_logits = torch.stack(clean_mle_residual_logits)\n",
    "    clean_target_residual_logits = torch.stack(clean_target_residual_logits)\n",
    "    corrupted_mle_residual_logits = torch.stack(corrupted_mle_residual_logits)\n",
    "    corrupted_target_residual_logits = torch.stack(corrupted_target_residual_logits)\n",
    "    ablated_mle_residual_logits = torch.stack(ablated_mle_residual_logits)\n",
    "    ablated_target_residual_logits = torch.stack(ablated_target_residual_logits)\n",
    "    \n",
    "    return (layer_names, \n",
    "            clean_mle_residual_logits,\n",
    "            clean_target_residual_logits,\n",
    "            corrupted_mle_residual_logits,\n",
    "            corrupted_target_residual_logits,\n",
    "            ablated_mle_residual_logits,\n",
    "            ablated_target_residual_logits)\n",
    "\n",
    "        \n",
    "def flatten_all_results():\n",
    "    pickle_files = [f for f in os.listdir(\"results\") if f.split(\".\")[-1]==\"pickle\"]\n",
    "    results = []\n",
    "    for file in pickle_files: \n",
    "        result = load(\"results/\" + file)\n",
    "        result = flatten_result(result)\n",
    "        results.append(result)\n",
    "                \n",
    "    layer_names = results[0][0]\n",
    "    clean_mle_residual_logits = torch.cat([r[1] for r in results])\n",
    "    clean_target_residual_logits = torch.cat([r[2] for r in results])\n",
    "    corrupted_mle_residual_logits = torch.cat([r[3] for r in results])\n",
    "    corrupted_target_residual_logits = torch.cat([r[4] for r in results])\n",
    "    ablated_mle_residual_logits  = torch.cat([r[5] for r in results])\n",
    "    ablated_target_residual_logits  = torch.cat([r[6] for r in results])\n",
    "    \n",
    "    return (layer_names, \n",
    "            clean_mle_residual_logits,\n",
    "            clean_target_residual_logits,\n",
    "            corrupted_mle_residual_logits,\n",
    "            corrupted_target_residual_logits,\n",
    "            ablated_mle_residual_logits,\n",
    "            ablated_target_residual_logits)\n",
    "    \n",
    "\n",
    "        \n",
    "(layer_names, \n",
    "clean_mle_residual_logits,\n",
    "clean_target_residual_logits,\n",
    "corrupted_mle_residual_logits,\n",
    "corrupted_target_residual_logits,\n",
    "ablated_mle_residual_logits,\n",
    "ablated_target_residual_logits) = flatten_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_delta_unembed_delta_ablate(layer_names, clean_residual_logits, corrupted_residual_logits, ablated_residual_logits, ablate_type=\"attn\", analysis_type=\"attn\"): \n",
    "    ablate_idx = torch.tensor([ablate_type in l for l in layer_names])\n",
    "    analysis_idx = torch.tensor([analysis_type in l for l in layer_names])\n",
    "    \n",
    "    clean_residual_logits = clean_residual_logits[:,analysis_idx]\n",
    "    corrupted_residual_logits = corrupted_residual_logits[:,analysis_idx]\n",
    "    ablated_residual_logits = ablated_residual_logits[:,:,analysis_idx]\n",
    "    \n",
    "    batch, ablate_idx, layer_idx = ablated_residual_logits.shape\n",
    "\n",
    "    delta_ablate_l = torch.zeros((batch, ablate_idx)) ; direct_effect = torch.zeros((batch, layer_idx))\n",
    "\n",
    "    for idx in range(ablate_idx): \n",
    "        delta_ablate_l[:,idx] = ablated_residual_logits[:,idx,:].sum(dim=-1) - clean_residual_logits.sum(dim=-1)\n",
    "        direct_effect[:,idx] = ablated_residual_logits[:,idx,idx] - clean_residual_logits[:,idx]\n",
    "    delta_unembed = clean_residual_logits\n",
    "\n",
    "    mean_delta_ablate = delta_ablate_l.mean(dim=0)\n",
    "    mean_direct_effect = direct_effect.mean(dim=0)\n",
    "    mean_delta_unembed = delta_unembed.mean(dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 6))\n",
    "    # Remove the last empty subplot from the grid\n",
    "    fig.delaxes(axes[1, 1])\n",
    "    fig.delaxes(axes[0,1])\n",
    "\n",
    "    \n",
    "    for exp_idx in range(batch): \n",
    "        axes[0,0].plot(delta_ablate_l[exp_idx], color=\"lightgrey\")\n",
    "        axes[1,0].plot(delta_unembed[exp_idx], color=\"lightgrey\")\n",
    "    axes[0,0].plot(mean_delta_ablate, color=\"blue\", label=\"Mean\")\n",
    "    axes[1,0].plot(mean_delta_unembed, color=\"blue\", label=\"Mean\")\n",
    "    \n",
    "    axes[0, 0].set_title(\"Delta Ablate\")\n",
    "    axes[1, 0].set_title(\"Delta Unembed\")\n",
    "\n",
    "    ax_third = fig.add_subplot(1, 2, 2)\n",
    "    for exp_idx in  range(batch): \n",
    "        da = delta_ablate_l[exp_idx]\n",
    "        du = delta_unembed[exp_idx]\n",
    "        layer_colors = plt.cm.viridis(np.linspace(0, 1, du.shape[-1]))\n",
    "        ax_third.scatter(x = du, y = da, c=layer_colors, s=10, marker='o', alpha=0.7)\n",
    "        \n",
    "    min_val = -2\n",
    "    max_val = 2\n",
    "    \n",
    "    magnitude_axis=2\n",
    "    ax_third.set_xlim(-magnitude_axis, magnitude_axis)\n",
    "    ax_third.set_ylim(-magnitude_axis, max_val)\n",
    "\n",
    "    # Set the limits for the dashed line to go through zero and extend on both sides\n",
    "    line_min = min(-2, 0)\n",
    "    line_max = max(6, 0)\n",
    "    ax_third.plot([line_min, line_max], [line_min, line_max], color='black', linestyle='dashed', alpha=0.5)\n",
    "    ax_third.set_title(\"Delta Ablate vs Delta Unembed\")\n",
    "    cax = fig.add_axes([0.99, 0.15, 0.02, 0.7])  # Adjust the position and size as needed\n",
    "    cbar = plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis), cax=cax)\n",
    "    tick_locations = np.linspace(0, 1, 8)  # Adjust the number of ticks as needed\n",
    "    tick_labels = np.arange(0, 36, 5)\n",
    "    cbar.set_ticks(tick_locations)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    cbar.set_label('Layer')\n",
    "    \n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_delta_unembed_delta_ablate(layer_names=layer_names,\n",
    "                    clean_residual_logits=clean_mle_residual_logits,\n",
    "                    corrupted_residual_logits=corrupted_mle_residual_logits,\n",
    "                    ablated_residual_logits=ablated_mle_residual_logits,\n",
    "                    ablate_type=\"attn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_compensatory_effect_by_layer(layer_names, clean_residual_logits, corrupted_residual_logits, ablated_residual_logits, ablate_type=\"attn\", analysis_type=\"attn\"):\n",
    "    # labels, target_residual_clean_logit, mle_residual_corrupted_logit, target_residual_clean_logit, mle_residual_clean_logit\n",
    "    ablate_idx = torch.tensor([ablate_type in l for l in layer_names])\n",
    "    analysis_idx = torch.tensor([analysis_type in l for l in layer_names])\n",
    "    disp_layer_names = [l for l in layer_names if analysis_type in l]\n",
    "    n_layers = analysis_idx.sum()\n",
    "    \n",
    "    clean_residual_logits = clean_residual_logits[:,analysis_idx]\n",
    "    corrupted_residual_logits = corrupted_residual_logits[:,analysis_idx]\n",
    "    ablated_residual_logits = ablated_residual_logits[:,:,analysis_idx]\n",
    "    \n",
    "    batch, ablate_idx, layer_idx = ablated_residual_logits.shape\n",
    "    \n",
    "    all_compensatory_effect = torch.zeros((batch, ablate_idx, n_layers))\n",
    "    delta_ablate_l = torch.zeros((batch, ablate_idx))\n",
    "    \n",
    "    for i in range(ablate_idx): \n",
    "        for j in range(n_layers): \n",
    "            # all_compensatory_effect[:,i,j] = ablated_residual_logits[:,i,j] - (corrupted_residual_logits[:,j] - clean_residual_logits[:,j])\n",
    "            all_compensatory_effect[:,i,j] = ablated_residual_logits[:,i,j] - corrupted_residual_logits[:,j] \n",
    "            \n",
    "            \n",
    "    all_compensatory_effect = all_compensatory_effect.mean(dim=0)\n",
    "    \n",
    "    \n",
    "    imshow(all_compensatory_effect.T,y=disp_layer_names,labels={\"x\":\"Ablation Index\", \"y\":\"Layer\"}, title=\"PLOT A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_compensatory_effect_by_layer(layer_names=layer_names,\n",
    "                    clean_residual_logits=clean_mle_residual_logits,\n",
    "                    corrupted_residual_logits=corrupted_mle_residual_logits,\n",
    "                    ablated_residual_logits=ablated_mle_residual_logits,\n",
    "                    ablate_type=\"attn\", \n",
    "                    analysis_type=\"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compensatory_effect_by_layer(results, attribution_type=\"attn\", ablation_type=\"attn\", use_target=True, subtract_clean_run=True):\n",
    "    # labels, target_residual_clean_logit, mle_residual_corrupted_logit, target_residual_clean_logit, mle_residual_clean_logit\n",
    "    layer_names = results[0][\"layer_names\"]\n",
    "    disp_layer_names = [l for l in layer_names if attribution_type in l]\n",
    "    idx_of_interest = torch.tensor([attribution_type in l in l for l in layer_names])\n",
    "    attn_idx = torch.tensor([\"attn\" in l in l for l in layer_names])\n",
    "    \n",
    "    n_ablations = results[0][\"ablate_direct_effect_on_target\"].shape[0]\n",
    "    \n",
    "    \n",
    "    all_compensatory_effect = torch.zeros((len(results), n_ablations, len(disp_layer_names)))    \n",
    "    \n",
    "    \n",
    "    for i,result in enumerate(results):\n",
    "        \n",
    "        if use_target: \n",
    "            ablate_direct_effect_all_layers = result[\"ablate_direct_effect_on_target\"]\n",
    "            unembed_direct_effect_all_layers = result[\"target_direct_effect\"]\n",
    "        else: \n",
    "            ablate_direct_effect_all_layers = result[\"ablate_direct_effect_on_mle\"]\n",
    "            unembed_direct_effect_all_layers = result[\"mle_direct_effect\"] \n",
    "            \n",
    "            \n",
    "        ablate_direct_effect = ablate_direct_effect_all_layers[:,idx_of_interest]\n",
    "        unembed_direct_effect = unembed_direct_effect_all_layers[idx_of_interest]\n",
    "\n",
    "        #calculate compensatory response\n",
    "        compensatory_response = torch.zeros((n_ablations, all_compensatory_effect.shape[-1]))\n",
    "        \n",
    "        for idx in range(n_ablations): \n",
    "            if subtract_clean_run: \n",
    "                ablate_direct_effect_all_layers = ablate_direct_effect[idx] - unembed_direct_effect\n",
    "            else: \n",
    "                ablate_direct_effect_all_layers = ablate_direct_effect[idx]\n",
    "\n",
    "            compensatory_response[idx] = (ablate_direct_effect_all_layers)\n",
    "                \n",
    "        \n",
    "        all_compensatory_effect[i] = compensatory_response\n",
    "    \n",
    "    all_compensatory_effect = all_compensatory_effect.mean(dim=0)\n",
    "    \n",
    "    imshow(all_compensatory_effect,y=disp_layer_names,labels={\"x\":\"Ablation Index\", \"y\":\"Layer\"}, title=\"add later\")\n",
    "    \n",
    "plot_compensatory_effect_by_layer(results_attn_CE, attribution_type=\"attn\", ablation_type=\"attn\", use_target=True, subtract_clean_run=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb812820b5094695c8a581672e17220e30dd2c15d704c018326e3cc2e1a566f1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03055d1e38844792bbd9750b2daf786a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0336fd949a66477c9daaca7b78b5a3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_669e15f191394c0ca74630c025b164bc",
       "IPY_MODEL_f32ea620bbc342dab3179e632f8fbe44",
       "IPY_MODEL_d276c9530d9f456e9ae9c0b3a931cb93"
      ],
      "layout": "IPY_MODEL_ea6138d3d54f483587595bdda5211215"
     }
    },
    "05c094717d594d37985f58c51527db59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0632336b81a043d09cab7a8b3774968c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f3cef4147924e129b11161067776df0",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03055d1e38844792bbd9750b2daf786a",
      "value": 124
     }
    },
    "08b2a881d8b44bca9eabbe25558d2b10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f3cef4147924e129b11161067776df0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1198949157d045b4b1d56330c21f861b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13e8123052ad4599829fc081dac4eab3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53ab2b48537d45f0a311d51bdf2f03ff",
      "placeholder": "​",
      "style": "IPY_MODEL_f36e03e930584b7d821b4184c54a5810",
      "value": " 456k/456k [00:00&lt;00:00, 9.60MB/s]"
     }
    },
    "1c3f06e311c648569f17cf18f9c2f4f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95713e453b7945d4bf3980f1ae512033",
      "placeholder": "​",
      "style": "IPY_MODEL_d767ae133f1a47bab5a40f4d8ac0d272",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "1ff7229e899e4afab9b5ee0b1597f5e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "267fca47895e493e82c9ef0568847b18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "272adc9fc3ba4c07943725eb800a18c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2df76b7b518948d88d9aba8f670e7f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30e53c8500a14fb880c2ab52007d1087": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32421abdc8fa41d8917203f6bfad586d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "340a9c92995a4189b69e459b033ae946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3419184787645f9b617dc18ccaf315a",
      "placeholder": "​",
      "style": "IPY_MODEL_45df25e66c6041d6b0470865f43d3030",
      "value": " 665/665 [00:00&lt;00:00, 27.5kB/s]"
     }
    },
    "4193f94efd9e457483fc4a2b03a3c07d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45df25e66c6041d6b0470865f43d3030": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a08436dc4c54a1ab2219e66058f68b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4baa43529767484fb04fb1f9e94c1aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4dd5d713b82742df9b120193752b769c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50fd7bca1a95484fa13c37242a988fc3",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6309b4c1e92f46e9b09f9615d20705d8",
      "value": 665
     }
    },
    "50fd7bca1a95484fa13c37242a988fc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ab2b48537d45f0a311d51bdf2f03ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5553fd97122a493fb99e3af859d16ddf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a08436dc4c54a1ab2219e66058f68b5",
      "placeholder": "​",
      "style": "IPY_MODEL_e75523e3b32a449bb5f08ab49d57a473",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "576faf57e7234808866d8ae0c273ecbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57a96c26b3944e7486bb2ae8c26d8973": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e5075a3f50b34526bdd4e215488341af",
       "IPY_MODEL_0632336b81a043d09cab7a8b3774968c",
       "IPY_MODEL_b5defcb6637e4847a76bb982bd40dba4"
      ],
      "layout": "IPY_MODEL_c21ba829e49b4824a83d9e79ec5b9892"
     }
    },
    "5f7b0750af3646b787026494a1872a1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6208163737e24a8ebc66fa95c7f4217f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6309b4c1e92f46e9b09f9615d20705d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6586f332f4104d8d914b8d390049310b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "669e15f191394c0ca74630c025b164bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1d36249ca9b430dad8d8327138b8389",
      "placeholder": "​",
      "style": "IPY_MODEL_a865c5a49b0f4e84a6621b16d682edbd",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "7413bf18522241d3807a01f016d6139e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e9d95a57d194b379f33467895fc8883": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7eecaf2e1f6d401b9d85abf1438cde9b",
       "IPY_MODEL_981666d6021845389f9fa1a0ff5c7997",
       "IPY_MODEL_d54bba9da6d54cbd8e83eec29dae02c4"
      ],
      "layout": "IPY_MODEL_2df76b7b518948d88d9aba8f670e7f9c"
     }
    },
    "7eecaf2e1f6d401b9d85abf1438cde9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2ce3a86d5b34b2eb263fedcbbce72a0",
      "placeholder": "​",
      "style": "IPY_MODEL_4baa43529767484fb04fb1f9e94c1aab",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "80bce29dfc73463c9b4cbeaea636ed74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e240513c73c2418f835465c3a93683f2",
      "placeholder": "​",
      "style": "IPY_MODEL_576faf57e7234808866d8ae0c273ecbd",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 16.1MB/s]"
     }
    },
    "89cc2257d1ea4e63a889c0c4c3b20b62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95713e453b7945d4bf3980f1ae512033": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9704b43fd02d47be994fefadbde6dfa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "981666d6021845389f9fa1a0ff5c7997": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1198949157d045b4b1d56330c21f861b",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32421abdc8fa41d8917203f6bfad586d",
      "value": 548105171
     }
    },
    "9b37ca3bdb944f2b95e65fdb4412e7dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb702efbe48642e1b9dd743faa0fcecf",
       "IPY_MODEL_c73ee2d1b39d4eeb90e976ee9710f859",
       "IPY_MODEL_13e8123052ad4599829fc081dac4eab3"
      ],
      "layout": "IPY_MODEL_272adc9fc3ba4c07943725eb800a18c1"
     }
    },
    "a865c5a49b0f4e84a6621b16d682edbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a90d112104994987b228ec0d4c7cd419": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2755e69b5484a28ade3d560840aecc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5defcb6637e4847a76bb982bd40dba4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6208163737e24a8ebc66fa95c7f4217f",
      "placeholder": "​",
      "style": "IPY_MODEL_30e53c8500a14fb880c2ab52007d1087",
      "value": " 124/124 [00:00&lt;00:00, 2.79kB/s]"
     }
    },
    "c21ba829e49b4824a83d9e79ec5b9892": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2ce3a86d5b34b2eb263fedcbbce72a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c55ebb7c205d40ba8c9f2682dd169f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ff7229e899e4afab9b5ee0b1597f5e5",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08b2a881d8b44bca9eabbe25558d2b10",
      "value": 1042301
     }
    },
    "c73ee2d1b39d4eeb90e976ee9710f859": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f073c1fbf7724c689cfe92ed50e43254",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d666d404db714d7f94a67d434b54a22b",
      "value": 456318
     }
    },
    "ce75e14ca9384d04bb297b6be28c36e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1d36249ca9b430dad8d8327138b8389": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d276c9530d9f456e9ae9c0b3a931cb93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce75e14ca9384d04bb297b6be28c36e5",
      "placeholder": "​",
      "style": "IPY_MODEL_267fca47895e493e82c9ef0568847b18",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 18.4MB/s]"
     }
    },
    "d3419184787645f9b617dc18ccaf315a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d54bba9da6d54cbd8e83eec29dae02c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c094717d594d37985f58c51527db59",
      "placeholder": "​",
      "style": "IPY_MODEL_9704b43fd02d47be994fefadbde6dfa2",
      "value": " 548M/548M [00:06&lt;00:00, 179MB/s]"
     }
    },
    "d5c7b0d2a6ff43eca128a12113300bd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5553fd97122a493fb99e3af859d16ddf",
       "IPY_MODEL_4dd5d713b82742df9b120193752b769c",
       "IPY_MODEL_340a9c92995a4189b69e459b033ae946"
      ],
      "layout": "IPY_MODEL_89cc2257d1ea4e63a889c0c4c3b20b62"
     }
    },
    "d666d404db714d7f94a67d434b54a22b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d767ae133f1a47bab5a40f4d8ac0d272": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e240513c73c2418f835465c3a93683f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5075a3f50b34526bdd4e215488341af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f7b0750af3646b787026494a1872a1f",
      "placeholder": "​",
      "style": "IPY_MODEL_6586f332f4104d8d914b8d390049310b",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "e75523e3b32a449bb5f08ab49d57a473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea6138d3d54f483587595bdda5211215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb702efbe48642e1b9dd743faa0fcecf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90d112104994987b228ec0d4c7cd419",
      "placeholder": "​",
      "style": "IPY_MODEL_ffb3be45dd1a4454bf59bb7ffb1e2413",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "f073c1fbf7724c689cfe92ed50e43254": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f32ea620bbc342dab3179e632f8fbe44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2755e69b5484a28ade3d560840aecc5",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7413bf18522241d3807a01f016d6139e",
      "value": 1355256
     }
    },
    "f36e03e930584b7d821b4184c54a5810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fba7c9b3bf9b409a9166cae7fc5fdba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c3f06e311c648569f17cf18f9c2f4f3",
       "IPY_MODEL_c55ebb7c205d40ba8c9f2682dd169f37",
       "IPY_MODEL_80bce29dfc73463c9b4cbeaea636ed74"
      ],
      "layout": "IPY_MODEL_4193f94efd9e457483fc4a2b03a3c07d"
     }
    },
    "ffb3be45dd1a4454bf59bb7ffb1e2413": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
